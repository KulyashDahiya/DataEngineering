{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T06:50:00.028571Z",
     "start_time": "2025-03-26T06:49:57.846894Z"
    }
   },
   "source": [
    "from pyspark.shell import spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 12:19:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.6 (default, Nov 11 2024 03:15:38)\n",
      "Spark context Web UI available at http://localhost:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1742971799845).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T06:50:00.514978Z",
     "start_time": "2025-03-26T06:50:00.189892Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"SparkTesting\").getOrCreate()",
   "id": "7fed1494fdaa4b2a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 12:20:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:45:30.339450Z",
     "start_time": "2025-03-20T10:45:30.235726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(\"This is a sentence.\",), (\"Another sentence here.\",), (\"Final example.\",)]\n",
    "df = spark.createDataFrame(data, [\"sentences\"])\n",
    "df.show(truncate=False)"
   ],
   "id": "30fe410391d72641",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sentences             |\n",
      "+----------------------+\n",
      "|This is a sentence.   |\n",
      "|Another sentence here.|\n",
      "|Final example.        |\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:45:31.895261Z",
     "start_time": "2025-03-20T10:45:31.750321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_words = df.withColumn(\"words\", F.explode(F.split(F.col(\"sentences\"), \" \")))\n",
    "df_words.show()"
   ],
   "id": "abde97da4af74a4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           sentences|    words|\n",
      "+--------------------+---------+\n",
      "| This is a sentence.|     This|\n",
      "| This is a sentence.|       is|\n",
      "| This is a sentence.|        a|\n",
      "| This is a sentence.|sentence.|\n",
      "|Another sentence ...|  Another|\n",
      "|Another sentence ...| sentence|\n",
      "|Another sentence ...|    here.|\n",
      "|      Final example.|    Final|\n",
      "|      Final example.| example.|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:16:17.870756Z",
     "start_time": "2025-03-20T11:16:17.830320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = [(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,)]\n",
    "df = spark.createDataFrame(data, [\"number\"])"
   ],
   "id": "289f20835e6497fa",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:18:46.491458Z",
     "start_time": "2025-03-20T11:18:46.486825Z"
    }
   },
   "cell_type": "code",
   "source": "df.rdd.getNumPartitions()",
   "id": "7d2d4eeb9a9fc0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:18:48.267517Z",
     "start_time": "2025-03-20T11:18:48.251014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_coalesced = df.coalesce(6)\n",
    "df_coalesced.rdd.getNumPartitions()"
   ],
   "id": "b6bbb942a126af02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:18:54.363427Z",
     "start_time": "2025-03-20T11:18:54.350595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_increased = df_coalesced.coalesce(10)\n",
    "df_increased.rdd.getNumPartitions()"
   ],
   "id": "12b116c59488b5f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T11:46:20.019887Z",
     "start_time": "2025-03-20T11:46:19.809715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField\n",
    "import random\n",
    "\n",
    "# Initialize Spark Session\n",
    "# spark = SparkSession.builder.appName(\"LargeDataFrame\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False),\n",
    "    StructField(\"department\", StringType(), False)\n",
    "])\n",
    "\n",
    "# Generate 10,000 rows of data\n",
    "data = [(i, f\"Name_{i}\", random.randint(18, 65), random.randint(30000, 120000),\n",
    "         random.choice([\"HR\", \"IT\", \"Finance\", \"Marketing\"])) for i in range(1, 10001)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df = df.coalesce(1)\n",
    "\n",
    "# Save DataFrame as Parquet file\n",
    "parquet_path = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/parquet_files/large_dataset_single.parquet\"\n",
    "df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "print(f\"Parquet file saved at: {parquet_path}\")\n"
   ],
   "id": "ba753f5bb28179d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file saved at: /Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/parquet_files/large_dataset_single.parquet\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:59:44.929355Z",
     "start_time": "2025-03-21T07:59:44.844116Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "d9185c539052231",
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/session.py:1796\u001B[0m, in \u001B[0;36mSparkSession.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1782\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1783\u001B[0m \u001B[38;5;124;03mStop the underlying :class:`SparkContext`.\u001B[39;00m\n\u001B[1;32m   1784\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1792\u001B[0m \u001B[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001B[39;00m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SQLContext\n\u001B[0;32m-> 1796\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1797\u001B[0m \u001B[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001B[39;00m\n\u001B[1;32m   1798\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/context.py:654\u001B[0m, in \u001B[0;36mSparkContext.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    653\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 654\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    655\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JError:\n\u001B[1;32m    656\u001B[0m         \u001B[38;5;66;03m# Case: SPARK-18523\u001B[39;00m\n\u001B[1;32m    657\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    658\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    659\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m It is possible that the process has crashed,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    660\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m been killed or may also be in a zombie state.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    661\u001B[0m             \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m,\n\u001B[1;32m    662\u001B[0m         )\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:01:01.232813Z",
     "start_time": "2025-03-21T08:01:01.038783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"JoinExample\").getOrCreate()\n",
    "\n",
    "# Create languages DataFrame\n",
    "data1 = [(\"1\", \"Java\", \"20000\"),\n",
    "         (\"2\", \"Python\", \"100000\"),\n",
    "         (\"3\", \"Scala\", \"3000\")]\n",
    "\n",
    "languages = spark.createDataFrame(data1, [\"id\", \"language\", \"tution_fees\"])\n",
    "languages.createOrReplaceTempView(\"languages\")\n",
    "\n",
    "# Create students DataFrame\n",
    "data2 = [(\"1\", \"studentA\"), (\"1\", \"studentB\"),\n",
    "         (\"2\", \"studentA\"), (\"3\", \"studentC\")]\n",
    "\n",
    "students = spark.createDataFrame(data2, [\"language_id\", \"studentName\"])\n",
    "students.createOrReplaceTempView(\"students\")\n",
    "\n",
    "# Perform join and aggregation using SQL\n",
    "df = spark.sql(\"\"\"\n",
    "    SELECT students.studentName, SUM(CAST(students.language_id AS INT)) as c\n",
    "    FROM students\n",
    "    INNER JOIN languages\n",
    "    ON students.language_id = languages.id\n",
    "    WHERE students.studentName = 'studentA'\n",
    "    GROUP BY students.studentName\n",
    "\"\"\")\n",
    "\n",
    "df.explain(extended=False)\n",
    "\n",
    "df.show()"
   ],
   "id": "88378d331328319b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[studentName#36], functions=[sum(cast(language_id#35 as int))])\n",
      "   +- Exchange hashpartitioning(studentName#36, 200), ENSURE_REQUIREMENTS, [plan_id=273]\n",
      "      +- HashAggregate(keys=[studentName#36], functions=[partial_sum(cast(language_id#35 as int))])\n",
      "         +- Project [language_id#35, studentName#36]\n",
      "            +- SortMergeJoin [language_id#35], [id#29], Inner\n",
      "               :- Sort [language_id#35 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(language_id#35, 200), ENSURE_REQUIREMENTS, [plan_id=265]\n",
      "               :     +- Filter ((isnotnull(studentName#36) AND (studentName#36 = studentA)) AND isnotnull(language_id#35))\n",
      "               :        +- Scan ExistingRDD[language_id#35,studentName#36]\n",
      "               +- Sort [id#29 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#29, 200), ENSURE_REQUIREMENTS, [plan_id=266]\n",
      "                     +- Project [id#29]\n",
      "                        +- Filter isnotnull(id#29)\n",
      "                           +- Scan ExistingRDD[id#29,language#30,tution_fees#31]\n",
      "\n",
      "\n",
      "+-----------+---+\n",
      "|studentName|  c|\n",
      "+-----------+---+\n",
      "|   studentA|  3|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:28:52.414190Z",
     "start_time": "2025-03-21T08:28:52.381502Z"
    }
   },
   "cell_type": "code",
   "source": "df.explain(extended=True)",
   "id": "856fd6808359259b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['students.studentName], ['students.studentName, 'SUM(cast('students.language_id as int)) AS c#39]\n",
      "+- 'Filter ('students.studentName = studentA)\n",
      "   +- 'Join Inner, ('students.language_id = 'languages.id)\n",
      "      :- 'UnresolvedRelation [students], [], false\n",
      "      +- 'UnresolvedRelation [languages], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "studentName: string, c: bigint\n",
      "Aggregate [studentName#36], [studentName#36, sum(cast(language_id#35 as int)) AS c#39L]\n",
      "+- Filter (studentName#36 = studentA)\n",
      "   +- Join Inner, (language_id#35 = id#29)\n",
      "      :- SubqueryAlias students\n",
      "      :  +- View (`students`, [language_id#35,studentName#36])\n",
      "      :     +- LogicalRDD [language_id#35, studentName#36], false\n",
      "      +- SubqueryAlias languages\n",
      "         +- View (`languages`, [id#29,language#30,tution_fees#31])\n",
      "            +- LogicalRDD [id#29, language#30, tution_fees#31], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [studentName#36], [studentName#36, sum(cast(language_id#35 as int)) AS c#39L]\n",
      "+- Project [language_id#35, studentName#36]\n",
      "   +- Join Inner, (language_id#35 = id#29)\n",
      "      :- Filter ((isnotnull(studentName#36) AND (studentName#36 = studentA)) AND isnotnull(language_id#35))\n",
      "      :  +- LogicalRDD [language_id#35, studentName#36], false\n",
      "      +- Project [id#29]\n",
      "         +- Filter isnotnull(id#29)\n",
      "            +- LogicalRDD [id#29, language#30, tution_fees#31], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[studentName#36], functions=[sum(cast(language_id#35 as int))], output=[studentName#36, c#39L])\n",
      "   +- Exchange hashpartitioning(studentName#36, 200), ENSURE_REQUIREMENTS, [plan_id=273]\n",
      "      +- HashAggregate(keys=[studentName#36], functions=[partial_sum(cast(language_id#35 as int))], output=[studentName#36, sum#44L])\n",
      "         +- Project [language_id#35, studentName#36]\n",
      "            +- SortMergeJoin [language_id#35], [id#29], Inner\n",
      "               :- Sort [language_id#35 ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(language_id#35, 200), ENSURE_REQUIREMENTS, [plan_id=265]\n",
      "               :     +- Filter ((isnotnull(studentName#36) AND (studentName#36 = studentA)) AND isnotnull(language_id#35))\n",
      "               :        +- Scan ExistingRDD[language_id#35,studentName#36]\n",
      "               +- Sort [id#29 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#29, 200), ENSURE_REQUIREMENTS, [plan_id=266]\n",
      "                     +- Project [id#29]\n",
      "                        +- Filter isnotnull(id#29)\n",
      "                           +- Scan ExistingRDD[id#29,language#30,tution_fees#31]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T09:03:26.655561Z",
     "start_time": "2025-03-21T09:03:26.556021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Access the JVM's SizeEstimator from PySpark\n",
    "df_jvm = df._jdf  # Get the Java DataFrame reference\n",
    "size_estimator = spark._jvm.org.apache.spark.util.SizeEstimator\n",
    "\n",
    "# Estimate size in bytes\n",
    "estimated_size_bytes = size_estimator.estimate(df_jvm)\n",
    "\n",
    "# Convert to MB\n",
    "estimated_size_mb = estimated_size_bytes / 1_000_000\n",
    "\n",
    "print(f\"Estimated size of the DataFrame = {estimated_size_mb:.2f} MB\")"
   ],
   "id": "246e7f12307462e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated size of the DataFrame = 4.79 MB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f6c88cfc35e72a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, upper, lower, to_date, date_format, regexp_replace, udf, substring\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType\n",
    "import datetime\n",
    "\n",
    "def apply_field_conversions(pDF, aAsset, aLogger=None):\n",
    "    df = pDF\n",
    "\n",
    "    def log(msg): aLogger.info(msg) if aLogger else print(msg)\n",
    "\n",
    "    def cast_to(target):\n",
    "        type_map = {\n",
    "            \"String\": StringType(),\n",
    "            \"Int\": IntegerType(),\n",
    "            \"Float\": FloatType(),\n",
    "            \"Date\": DateType()\n",
    "        }\n",
    "        return lambda c: c.cast(type_map[target])\n",
    "\n",
    "    def date_to_days_udf():\n",
    "        def date_to_days(d):\n",
    "            if d:\n",
    "                base_date = datetime.date(1900, 1, 1)\n",
    "                if isinstance(d, datetime.datetime):\n",
    "                    d = d.date()\n",
    "                return (d - base_date).days if isinstance(d, datetime.date) else None\n",
    "            return None\n",
    "        return udf(date_to_days, IntegerType())\n",
    "\n",
    "    def get_handler(rule, source_type, target_type):\n",
    "        rule = rule.strip()\n",
    "\n",
    "        if rule.startswith(\"Left:\"):\n",
    "            n = int(rule.split(\"Left:\")[1])\n",
    "            return lambda c: substring(c, 1, n)\n",
    "\n",
    "        if rule == \"???\":\n",
    "            rule = \"Currency:Dollar\"\n",
    "\n",
    "        if rule.startswith(\"Date:\"):\n",
    "            fmt = rule.split(\"Date:\")[1]\n",
    "            if source_type == \"String\":\n",
    "                return lambda c: to_date(c, fmt)\n",
    "            elif source_type == \"Date\":\n",
    "                return lambda c: date_format(c, fmt)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid source type for rule {rule}: {source_type}\")\n",
    "\n",
    "        handler_map = {\n",
    "            \"Upper\": lambda c: upper(c),\n",
    "            \"Lower\": lambda c: lower(c),\n",
    "            \"Decimal:Comma\": lambda c: regexp_replace(c, \",\", \".\").cast(FloatType()),\n",
    "            \"Currency:Dollar\": lambda c: regexp_replace(c, \"[$]\", \"\").cast(FloatType()),\n",
    "            \"Standard\": cast_to(target_type)\n",
    "        }\n",
    "\n",
    "        if rule not in handler_map:\n",
    "            raise ValueError(f\"Unsupported rule: {rule}\")\n",
    "\n",
    "        return handler_map[rule]\n",
    "\n",
    "    # --- Process Each Field ---\n",
    "    log(\"Starting conversion process...\")\n",
    "\n",
    "    for dField in aAsset[\"Fields\"]:\n",
    "        field = dField['Target_Field_Name']\n",
    "        source_type = dField['Source_Data_Type_Code']\n",
    "        target_type = dField['Target_Data_Type_Code']\n",
    "        rule_string = dField.get('Conversion_Rule_String', 'Standard')\n",
    "        rule_list = [r.strip() for r in rule_string.split(',')]\n",
    "\n",
    "        log(f\"\\nField: {field} | Source: {source_type} ➡ Target: {target_type} | Rules: {rule_list}\")\n",
    "\n",
    "        try:\n",
    "            expr = col(field)\n",
    "            for r in rule_list:\n",
    "                handler = get_handler(r, source_type, target_type)\n",
    "                expr = handler(expr)\n",
    "                log(f\"  ↳ Applied: {r}\")\n",
    "            df = df.withColumn(field, expr)\n",
    "            log(f\"[Success] Field transformed: {field}\")\n",
    "        except Exception as e:\n",
    "            log(f\"[Error] Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "    log(\"Conversion process complete.\")\n",
    "    return df\n"
   ],
   "id": "d6f22035c27477b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:54:59.758319Z",
     "start_time": "2025-03-27T06:54:59.744665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, upper, lower, to_date, date_format, regexp_replace, udf, substring\n",
    ")\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def apply_field_conversions(pDF, aAsset, aLogger=None):\n",
    "    df = pDF\n",
    "\n",
    "    def log(msg): aLogger.info(msg) if aLogger else print(msg)\n",
    "\n",
    "    def cast_to(target_type):\n",
    "        type_map = {\n",
    "            \"String\": StringType(),\n",
    "            \"Int\": IntegerType(),\n",
    "            \"Float\": FloatType(),\n",
    "            \"Date\": DateType()\n",
    "        }\n",
    "        return lambda c: c.cast(type_map[target_type])\n",
    "\n",
    "    def date_to_days_udf():\n",
    "        def date_to_days(d):\n",
    "            if d:\n",
    "                base = datetime.date(1900, 1, 1)\n",
    "                if isinstance(d, datetime.datetime):\n",
    "                    d = d.date()\n",
    "                return (d - base).days if isinstance(d, datetime.date) else None\n",
    "            return None\n",
    "        return udf(date_to_days, IntegerType())\n",
    "\n",
    "    def get_handler(rule, source_type, target_type):\n",
    "        rule = rule.strip()\n",
    "\n",
    "        # Left:N substring rule\n",
    "        if rule.lower().startswith(\"left:\"):\n",
    "            n = int(rule.split(\":\")[1])\n",
    "            return lambda c: substring(c, 1, n)\n",
    "\n",
    "        # Currency cleanup\n",
    "        if rule == \"???\":\n",
    "            rule = \"Currency:Dollar\"\n",
    "\n",
    "        # Date parsing/formatting\n",
    "        if rule.startswith(\"Date:\"):\n",
    "            fmt = rule.split(\"Date:\")[1]\n",
    "\n",
    "            if source_type == \"String\" and target_type == \"Date\":\n",
    "                return lambda c: to_date(c, fmt)\n",
    "            elif source_type == \"Date\" and target_type == \"String\":\n",
    "                return lambda c: date_format(c, fmt)\n",
    "            elif source_type == \"String\" and target_type == \"String\":\n",
    "                return lambda c: date_format(to_date(c, 'yyyy-MM-dd'), fmt)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Date conversion: {source_type} ➡ {target_type}\")\n",
    "\n",
    "        # Static conversion rules\n",
    "        handler_map = {\n",
    "            \"Upper\": lambda c: upper(c),\n",
    "            \"Lower\": lambda c: lower(c),\n",
    "            \"Decimal:Comma\": lambda c: round(regexp_replace(c, \",\", \".\").cast(FloatType()),2),\n",
    "            \"Currency:Dollar\": lambda c: round(regexp_replace(c, \"[$]\", \"\").cast(FloatType()),2),\n",
    "            \"DateToInt\": lambda c: date_to_days_udf()(c),\n",
    "            \"Standard\": cast_to(target_type)\n",
    "        }\n",
    "\n",
    "        if rule not in handler_map:\n",
    "            raise ValueError(f\"Unsupported rule: {rule}\")\n",
    "\n",
    "        return handler_map[rule]\n",
    "\n",
    "    # Apply rules\n",
    "    log(\"🔄 Starting field conversion...\")\n",
    "\n",
    "    for dField in aAsset[\"Fields\"]:\n",
    "        field = dField[\"Target_Field_Name\"]\n",
    "        source_type = dField[\"Source_Data_Type_Code\"]\n",
    "        target_type = dField[\"Target_Data_Type_Code\"]\n",
    "        rule_string = dField.get(\"Conversion_Rule_String\", \"Standard\")\n",
    "        rule_list = [r.strip() for r in rule_string.split(\",\")]\n",
    "\n",
    "        log(f\"\\n🛠️  Field: {field} | Source: {source_type} ➡ Target: {target_type} | Rules: {rule_list}\")\n",
    "\n",
    "        try:\n",
    "            expr = col(field)\n",
    "            for rule in rule_list:\n",
    "                handler = get_handler(rule, source_type, target_type)\n",
    "                expr = handler(expr)\n",
    "                log(f\"   ↪️ Applied: {rule}\")\n",
    "            df = df.withColumn(field, expr)\n",
    "            log(f\"✅ Transformed: {field}\")\n",
    "        except Exception as e:\n",
    "            log(f\"❌ [Error] Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "    log(\"✅ Field conversion complete.\")\n",
    "    return df"
   ],
   "id": "42ac860bc8ed75f5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:55:05.301011Z",
     "start_time": "2025-03-27T06:55:05.205737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, upper, lower, to_date, date_format,\n",
    "    regexp_replace, udf, substring, round\n",
    ")\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType, StructType, StructField\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Setup Spark\n",
    "spark = SparkSession.builder.appName(\"FieldConversionTest\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"StandardStr\", StringType(), True),\n",
    "    StructField(\"UpperStr\", StringType(), True),\n",
    "    StructField(\"LowerStr\", StringType(), True),\n",
    "    StructField(\"YMDStr\", StringType(), True),\n",
    "    StructField(\"MDYStr\", StringType(), True),\n",
    "    StructField(\"IntStr\", StringType(), True),\n",
    "    StructField(\"FloatStr\", StringType(), True),\n",
    "    StructField(\"CommaStr\", StringType(), True),\n",
    "    StructField(\"DollarStr\", StringType(), True),\n",
    "    StructField(\"IntVal\", IntegerType(), True),\n",
    "    StructField(\"FloatVal\", FloatType(), True),\n",
    "    StructField(\"DateCol\", DateType(), True),        # True Date value\n",
    "    StructField(\"DateTimeStr\", StringType(), True)   # DateTime string for Left:10\n",
    "])\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "data = [(\n",
    "    \"ApPLE\",               # StandardStr\n",
    "    \"Apple\",               # UpperStr\n",
    "    \"Apple\",               # LowerStr\n",
    "    \"2025-05-25\",          # YMDStr\n",
    "    \"05/25/2025\",          # MDYStr\n",
    "    \"123\",                 # IntStr\n",
    "    \"123.45\",              # FloatStr\n",
    "    \"123,45\",              # CommaStr\n",
    "    \"$123.45\",             # DollarStr\n",
    "    2,                     # IntVal\n",
    "    123.45,                # FloatVal\n",
    "    date(2025, 5, 25),     # DateCol\n",
    "    \"2025-05-25 12:34:56\"  # DateTimeStr\n",
    ")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "aAsset = {\n",
    "    \"Fields\": [\n",
    "        {\"Target_Field_Name\": \"StandardStr\",   \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"UpperStr\",      \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Upper\"},\n",
    "        {\"Target_Field_Name\": \"LowerStr\",      \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Lower\"},\n",
    "        {\"Target_Field_Name\": \"YMDStr\",        \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\",   \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"MDYStr\",        \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\",   \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"},\n",
    "        {\"Target_Field_Name\": \"IntStr\",        \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Int\",    \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"FloatStr\",      \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\",  \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"CommaStr\",      \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\",  \"Conversion_Rule_String\": \"Decimal:Comma\"},\n",
    "        {\"Target_Field_Name\": \"DollarStr\",     \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\",  \"Conversion_Rule_String\": \"???\"},\n",
    "        {\"Target_Field_Name\": \"IntVal\",        \"Source_Data_Type_Code\": \"Int\",    \"Target_Data_Type_Code\": \"Int\",    \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"IntVal\",        \"Source_Data_Type_Code\": \"Int\",    \"Target_Data_Type_Code\": \"Float\",  \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"FloatVal\",      \"Source_Data_Type_Code\": \"Float\",  \"Target_Data_Type_Code\": \"Float\",  \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"DateCol\",       \"Source_Data_Type_Code\": \"Date\",   \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        # {\"Target_Field_Name\": \"DateCol\",       \"Source_Data_Type_Code\": \"Date\",   \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"},\n",
    "        {\"Target_Field_Name\": \"DateTimeStr\",   \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\",   \"Conversion_Rule_String\": \"Left:10, Date:yyyy-MM-dd\"}\n",
    "    ]\n",
    "}\n"
   ],
   "id": "da018a8ece11021f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateCol   |DateTimeStr        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-------------------+\n",
      "|ApPLE      |Apple   |Apple   |2025-05-25|05/25/2025|123   |123.45  |123,45  |$123.45  |2     |123.45  |2025-05-25|2025-05-25 12:34:56|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:55:10.789438Z",
     "start_time": "2025-03-27T06:55:10.520919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Apply conversion\n",
    "converted_df = apply_field_conversions(pDF=df, aAsset=aAsset)\n",
    "\n",
    "# Show results\n",
    "converted_df.show(truncate=False)\n",
    "converted_df.printSchema()"
   ],
   "id": "3665ce105fa99c5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting field conversion...\n",
      "\n",
      "🛠️  Field: StandardStr | Source: String ➡ Target: String | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: StandardStr\n",
      "\n",
      "🛠️  Field: UpperStr | Source: String ➡ Target: String | Rules: ['Upper']\n",
      "   ↪️ Applied: Upper\n",
      "✅ Transformed: UpperStr\n",
      "\n",
      "🛠️  Field: LowerStr | Source: String ➡ Target: String | Rules: ['Lower']\n",
      "   ↪️ Applied: Lower\n",
      "✅ Transformed: LowerStr\n",
      "\n",
      "🛠️  Field: YMDStr | Source: String ➡ Target: Date | Rules: ['Date:yyyy-MM-dd']\n",
      "   ↪️ Applied: Date:yyyy-MM-dd\n",
      "✅ Transformed: YMDStr\n",
      "\n",
      "🛠️  Field: MDYStr | Source: String ➡ Target: Date | Rules: ['Date:MM/dd/yyyy']\n",
      "   ↪️ Applied: Date:MM/dd/yyyy\n",
      "✅ Transformed: MDYStr\n",
      "\n",
      "🛠️  Field: IntStr | Source: String ➡ Target: Int | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: IntStr\n",
      "\n",
      "🛠️  Field: FloatStr | Source: String ➡ Target: Float | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: FloatStr\n",
      "\n",
      "🛠️  Field: CommaStr | Source: String ➡ Target: Float | Rules: ['Decimal:Comma']\n",
      "   ↪️ Applied: Decimal:Comma\n",
      "✅ Transformed: CommaStr\n",
      "\n",
      "🛠️  Field: DollarStr | Source: String ➡ Target: Float | Rules: ['???']\n",
      "   ↪️ Applied: ???\n",
      "✅ Transformed: DollarStr\n",
      "\n",
      "🛠️  Field: IntVal | Source: Int ➡ Target: Int | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: IntVal\n",
      "\n",
      "🛠️  Field: IntVal | Source: Int ➡ Target: Float | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: IntVal\n",
      "\n",
      "🛠️  Field: FloatVal | Source: Float ➡ Target: Float | Rules: ['Standard']\n",
      "   ↪️ Applied: Standard\n",
      "✅ Transformed: FloatVal\n",
      "\n",
      "🛠️  Field: DateCol | Source: Date ➡ Target: String | Rules: ['Date:yyyy-MM-dd']\n",
      "   ↪️ Applied: Date:yyyy-MM-dd\n",
      "✅ Transformed: DateCol\n",
      "\n",
      "🛠️  Field: DateTimeStr | Source: String ➡ Target: Date | Rules: ['Left:10', 'Date:yyyy-MM-dd']\n",
      "   ↪️ Applied: Left:10\n",
      "   ↪️ Applied: Date:yyyy-MM-dd\n",
      "✅ Transformed: DateTimeStr\n",
      "✅ Field conversion complete.\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateCol   |DateTimeStr|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "|ApPLE      |APPLE   |apple   |2025-05-25|2025-05-25|123   |123.45  |123.45  |123.45   |2.0   |123.45  |2025-05-25|2025-05-25 |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "\n",
      "root\n",
      " |-- StandardStr: string (nullable = true)\n",
      " |-- UpperStr: string (nullable = true)\n",
      " |-- LowerStr: string (nullable = true)\n",
      " |-- YMDStr: date (nullable = true)\n",
      " |-- MDYStr: date (nullable = true)\n",
      " |-- IntStr: integer (nullable = true)\n",
      " |-- FloatStr: float (nullable = true)\n",
      " |-- CommaStr: float (nullable = true)\n",
      " |-- DollarStr: float (nullable = true)\n",
      " |-- IntVal: float (nullable = true)\n",
      " |-- FloatVal: float (nullable = true)\n",
      " |-- DateCol: string (nullable = true)\n",
      " |-- DateTimeStr: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:55:13.367697Z",
     "start_time": "2025-03-27T06:55:13.288125Z"
    }
   },
   "cell_type": "code",
   "source": "converted_df.show(truncate=False)",
   "id": "7cedb772c1687af6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateCol   |DateTimeStr|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "|ApPLE      |APPLE   |apple   |2025-05-25|2025-05-25|123   |123.45  |123.45  |123.45   |2.0   |123.45  |2025-05-25|2025-05-25 |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T08:13:34.605791Z",
     "start_time": "2025-03-26T08:13:34.541025Z"
    }
   },
   "cell_type": "code",
   "source": "df.select(\"DashesDateStr\").show(truncate=False)",
   "id": "374d7868141ede3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|DashesDateStr|\n",
      "+-------------+\n",
      "|05-25-2025   |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:54:04.934534Z",
     "start_time": "2025-03-27T06:54:04.691301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample data with actual datetime objects\n",
    "data = [\n",
    "    [\"Hello World\", \"hello world\", \"HELLO WORLD\", \"LeftFiveTest\", \"123,45\", \"$1,234.56\", \"2024-12-25\", \"25-12-2024\", datetime(2024, 12, 25)],\n",
    "    [\"Example\", \"example\", \"EXAMPLE\", \"ABCDE12345\", \"456,78\", \"$987.65\", \"2025-01-01\", \"01-01-2025\", datetime(2025, 1, 1)],\n",
    "    [\"Test String\", \"test string\", \"TEST STRING\", \"SparkTestVal\", \"789,01\", \"$55.40\", \"2023-06-15\", \"15-06-2023\", datetime(2023, 6, 15)],\n",
    "    [\"Data Here\", \"data here\", \"DATA HERE\", \"Val12345\", \"321,99\", \"$10,000.99\", \"2022-11-11\", \"11-11-2022\", datetime(2022, 11, 11)],\n",
    "    [\"TextLine\", \"textline\", \"TEXTLINE\", \"AbcdeXyz\", \"98,76\", \"$65.00\", \"2020-02-29\", \"29-02-2020\", datetime(2020, 2, 29)],\n",
    "    [\"Entry\", \"entry\", \"ENTRY\", \"Five5Five\", \"100,00\", \"$5.50\", \"2021-01-01\", \"01-01-2021\", datetime(2021, 1, 1)],\n",
    "    [\"MoreText\", \"moretext\", \"MORETEXT\", \"SubStrTest\", \"888,88\", \"$88.88\", \"2026-08-08\", \"08-08-2026\", datetime(2026, 8, 8)],\n",
    "    [\"Sample\", \"sample\", \"SAMPLE\", \"TestValue\", \"1,23\", \"$1.23\", \"2027-07-07\", \"07-07-2027\", datetime(2027, 7, 7)],\n",
    "    [\"Alpha\", \"alpha\", \"ALPHA\", \"12345abcde\", \"3,14\", \"$3.14\", \"2028-09-09\", \"09-09-2028\", datetime(2028, 9, 9)],\n",
    "    [\"Final Row\", \"final row\", \"FINAL ROW\", \"FinalTest\", \"0,99\", \"$0.99\", \"2029-10-10\", \"10-10-2029\", datetime(2029, 10, 10)],\n",
    "]\n",
    "\n",
    "# Column headers\n",
    "columns = [\n",
    "    \"Standard_String\",\n",
    "    \"Upper_String\",\n",
    "    \"Lower_String\",\n",
    "    \"Left5_String\",\n",
    "    \"Decimal_Comma\",\n",
    "    \"Currency_Dollar\",\n",
    "    \"Date_ISO\",\n",
    "    \"Date_DDMMYYYY\",\n",
    "    \"DateToInt_Test\"  # This will be a true date field\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Output file path\n",
    "output_file = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/data/Test_Field_Conversion.csv\"\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ CSV file saved as: {output_file}\")"
   ],
   "id": "2d651341f5d6f3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV file saved as: /Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/data/Test_Field_Conversion.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:21:00.606252Z",
     "start_time": "2025-03-26T09:21:00.601535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.listdir(\"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/data/\")"
   ],
   "id": "dab66b2ce6e6b896",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'spark_columns',\n",
       " 'TimeProvince.csv',\n",
       " 'sparkdf',\n",
       " 'Region.csv',\n",
       " 'Case.csv']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "859ce463b3e20805"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "515c1a2ac5c2b661"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:39:42.281910Z",
     "start_time": "2025-03-28T05:39:42.112759Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2b0785fdba12fd5c",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DatabaseConstants'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mDatabaseConstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TableNames,ProjectColumns,AssetGroupColumns,AssetColumns,FieldColumns,IncrementalLoadColumns,AssetHistoryColumns,AssetStatusColumns,HistoricalLoadColumns,HolidayCalendarColumns\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# This holds all the flags for each Load Type\u001B[39;00m\n\u001B[1;32m     11\u001B[0m G_TempSuffix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_temp\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'DatabaseConstants'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "559dc3a7e7400e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "435e344fac18ff84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T05:38:26.808485Z",
     "start_time": "2025-03-28T05:38:20.811163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateConversionTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test Data with String Dates, Actual Dates, and Mixed Inputs\n",
    "test_data = [\n",
    "    Row(Date_Actual=\"2025-05-25\", Date_ISO=\"2025-05-25\", Date_MDY=\"05/25/2025\"),\n",
    "    Row(Date_Actual=\"2024-12-31\", Date_ISO=\"2024-12-31\", Date_MDY=\"12/31/2024\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(test_data)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Asset configuration with Conversion Rules\n",
    "aAsset = {\n",
    "    \"Fields\": [\n",
    "        # String to Date Conversion\n",
    "        {\n",
    "            \"Target_Field_Name\": \"Date_ISO\",\n",
    "            \"Source_Data_Type_Code\": \"String\",\n",
    "            \"Target_Data_Type_Code\": \"Date\",\n",
    "            \"Conversion_Rule_String\": \"Date:YYYY-MM-DD\"\n",
    "        },\n",
    "        {\n",
    "            \"Target_Field_Name\": \"Date_MDY\",\n",
    "            \"Source_Data_Type_Code\": \"String\",\n",
    "            \"Target_Data_Type_Code\": \"Date\",\n",
    "            \"Conversion_Rule_String\": \"Date:MM/DD/YYYY\"\n",
    "        },\n",
    "\n",
    "        # Date to String Conversion\n",
    "        {\n",
    "            \"Target_Field_Name\": \"Date_Actual\",\n",
    "            \"Source_Data_Type_Code\": \"Date\",\n",
    "            \"Target_Data_Type_Code\": \"String\",\n",
    "            \"Conversion_Rule_String\": \"Date:YYYY-MM-DD\"\n",
    "        },\n",
    "        {\n",
    "            \"Target_Field_Name\": \"Date_Actual\",\n",
    "            \"Source_Data_Type_Code\": \"Date\",\n",
    "            \"Target_Data_Type_Code\": \"String\",\n",
    "            \"Conversion_Rule_String\": \"Date:MM/DD/YYYY\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Apply Conversion\n",
    "converted_df = applyFieldConversions(df, aAsset)\n",
    "\n",
    "# Show Result\n",
    "converted_df.show(truncate=False)"
   ],
   "id": "3763566866c123a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/28 11:08:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n",
      "|Date_Actual|Date_ISO  |Date_MDY  |\n",
      "+-----------+----------+----------+\n",
      "|2025-05-25 |2025-05-25|05/25/2025|\n",
      "|2024-12-31 |2024-12-31|12/31/2024|\n",
      "+-----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DIFProcessedTableHelper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 53\u001B[0m\n\u001B[1;32m     20\u001B[0m aAsset \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFields\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;66;03m# String to Date Conversion\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m     ]\n\u001B[1;32m     50\u001B[0m }\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# Apply Conversion\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m helper \u001B[38;5;241m=\u001B[39m \u001B[43mDIFProcessedTableHelper\u001B[49m(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m, spark, {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoad_Type_Code\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT1-All-Replace\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[1;32m     54\u001B[0m converted_df \u001B[38;5;241m=\u001B[39m helper\u001B[38;5;241m.\u001B[39mapplyFieldConversions(df, aAsset)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Show Result\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'DIFProcessedTableHelper' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7d71a51ba79d4fc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
