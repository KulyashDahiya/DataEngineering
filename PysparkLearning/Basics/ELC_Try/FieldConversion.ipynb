{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:57:50.951844Z",
     "start_time": "2025-03-28T07:57:50.938352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, upper, lower, to_date, date_format, regexp_replace, udf, substring\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def applyFieldConversions(pDF, aAsset):\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "\n",
    "    df = pDF\n",
    "\n",
    "    def cast_to(target_type):\n",
    "        type_map = {\n",
    "            \"String\": StringType(),\n",
    "            \"Int\": IntegerType(),\n",
    "            \"Float\": FloatType(),\n",
    "            \"Date\": DateType()\n",
    "        }\n",
    "        return lambda c: c.cast(type_map[target_type])\n",
    "\n",
    "    def date_to_days_udf():\n",
    "        def date_to_days(d):\n",
    "            if d:\n",
    "                base = datetime.date(1900, 1, 1)\n",
    "                if isinstance(d, datetime.datetime):\n",
    "                    d = d.date()\n",
    "                return (d - base).days if isinstance(d, datetime.date) else None\n",
    "            return None\n",
    "        return udf(date_to_days, IntegerType())\n",
    "\n",
    "    def get_handler(rule, source_type, target_type):\n",
    "        rule = rule.strip()\n",
    "\n",
    "        # Left:N substring rule\n",
    "        if rule.lower().startswith(\"left:\"):\n",
    "            n = int(rule.split(\":\")[1])\n",
    "            return lambda c: substring(c, 1, n)\n",
    "\n",
    "        # Currency cleanup\n",
    "        if rule == \"???\":\n",
    "            rule = \"Currency:Dollar\"\n",
    "\n",
    "        # Date parsing/formatting\n",
    "        if rule.startswith(\"Date:\"):\n",
    "            fmt = rule.split(\"Date:\")[1]\n",
    "\n",
    "            if source_type == \"String\" and target_type == \"Date\":\n",
    "                return lambda c: to_date(c, fmt)\n",
    "            elif source_type == \"Date\" and target_type == \"String\":\n",
    "                return lambda c: date_format(c, fmt)\n",
    "            elif source_type == \"String\" and target_type == \"String\":\n",
    "                return lambda c: date_format(to_date(c, 'yyyy-MM-dd'), fmt)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Date conversion: {source_type} to {target_type}\")\n",
    "\n",
    "        # Static conversion rules\n",
    "        handler_map = {\n",
    "            \"Upper\": lambda c: upper(c),\n",
    "            \"Lower\": lambda c: lower(c),\n",
    "            \"Decimal:Comma\": lambda c: regexp_replace(c, \",\", \".\").cast(FloatType()),\n",
    "            \"Currency:Dollar\": lambda c: regexp_replace(regexp_replace(c, \"[$]\", \"\"), \",\", \"\").cast(FloatType()),\n",
    "            \"DateToInt\": lambda c: date_to_days_udf()(c),\n",
    "            \"Standard\": cast_to(target_type)\n",
    "        }\n",
    "\n",
    "        if rule not in handler_map:\n",
    "            raise ValueError(f\"Unsupported rule: {rule}\")\n",
    "\n",
    "        return handler_map[rule]\n",
    "\n",
    "    # Apply rules\n",
    "    print(\"Starting field conversion...\")\n",
    "\n",
    "    for dField in aAsset[\"Fields\"]:\n",
    "        field = dField[\"Target_Field_Name\"]\n",
    "        source_type = dField[\"Source_Data_Type_Code\"]\n",
    "        target_type = dField[\"Target_Data_Type_Code\"]\n",
    "        rule_string = dField.get(\"Conversion_Rule_String\", \"Standard\")\n",
    "        rule_list = [r.strip() for r in rule_string.split(\",\")]\n",
    "\n",
    "        print(f\"\\n Field: {field} | Source: {source_type} to  Target: {target_type} | Rules: {rule_list}\")\n",
    "\n",
    "        try:\n",
    "            expr = col(field)\n",
    "            for rule in rule_list:\n",
    "                handler = get_handler(rule, source_type, target_type)\n",
    "                expr = handler(expr)\n",
    "                print(f\"Applied: {rule}\")\n",
    "            df = df.withColumn(field, expr)\n",
    "            print(f\"Transformed: {field}\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error - Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "    print(\"Field conversion complete.\")\n",
    "    return df"
   ],
   "id": "2f4a16b64c6941e1",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:47:50.925525Z",
     "start_time": "2025-03-28T06:47:50.915124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, upper, lower, to_date, date_format, regexp_replace, udf, substring\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def applyFieldConversions(pDF, aAsset):\n",
    "\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "\n",
    "    df = pDF\n",
    "\n",
    "    def cast_to(target_type):\n",
    "        type_map = {\n",
    "            \"String\": StringType(),\n",
    "            \"Int\": IntegerType(),\n",
    "            \"Float\": FloatType(),\n",
    "            \"Date\": DateType()\n",
    "        }\n",
    "        return lambda c: c.cast(type_map[target_type])\n",
    "\n",
    "    def date_to_days_udf():\n",
    "        def date_to_days(d):\n",
    "            if d:\n",
    "                base = datetime.date(1900, 1, 1)\n",
    "                if isinstance(d, datetime.datetime):\n",
    "                    d = d.date()\n",
    "                return (d - base).days if isinstance(d, datetime.date) else None\n",
    "            return None\n",
    "        return udf(date_to_days, IntegerType())\n",
    "\n",
    "    def get_handler(rule, source_type, target_type):\n",
    "        rule = rule.strip()\n",
    "\n",
    "        # Left:N substring rule\n",
    "        if rule.lower().startswith(\"left:\"):\n",
    "            n = int(rule.split(\":\")[1])\n",
    "            return lambda c: substring(c, 1, n)\n",
    "\n",
    "        # Currency cleanup\n",
    "        if rule == \"???\":\n",
    "            rule = \"Currency:Dollar\"\n",
    "\n",
    "        # Date parsing/formatting\n",
    "        if rule.startswith(\"Date:\"):\n",
    "            fmt = rule.split(\"Date:\")[1]\n",
    "\n",
    "            if source_type == \"String\" and target_type == \"Date\":\n",
    "                return lambda c: to_date(c, fmt)\n",
    "            elif source_type == \"Date\" and target_type == \"String\":\n",
    "                return lambda c: date_format(c, fmt)\n",
    "            elif source_type == \"String\" and target_type == \"String\":\n",
    "                return lambda c: date_format(to_date(c, 'yyyy-MM-dd'), fmt)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Date conversion: {source_type} to {target_type}\")\n",
    "\n",
    "        # Static conversion rules\n",
    "        handler_map = {\n",
    "            \"Upper\": lambda c: upper(c),\n",
    "            \"Lower\": lambda c: lower(c),\n",
    "            \"Decimal:Comma\": lambda c: regexp_replace(c, \",\", \".\").cast(FloatType()),\n",
    "            \"Currency:Dollar\": lambda c: regexp_replace(regexp_replace(c, \"[$]\", \"\"), \",\", \"\").cast(FloatType()),\n",
    "            \"DateToInt\": lambda c: date_to_days_udf()(c),\n",
    "            \"Standard\": cast_to(target_type)\n",
    "        }\n",
    "\n",
    "        if rule not in handler_map:\n",
    "            raise ValueError(f\"Unsupported rule: {rule}\")\n",
    "\n",
    "        return handler_map[rule]\n",
    "\n",
    "    # Apply rules\n",
    "    print(\"Starting field conversion...\")\n",
    "\n",
    "    for dField in aAsset[\"Fields\"]:\n",
    "        field = dField[\"Target_Field_Name\"]\n",
    "        source_type = dField[\"Source_Data_Type_Code\"]\n",
    "        target_type = dField[\"Target_Data_Type_Code\"]\n",
    "        rule_string = dField.get(\"Conversion_Rule_String\", \"Standard\")\n",
    "        rule_list = [r.strip() for r in rule_string.split(\",\")]\n",
    "\n",
    "        print(f\"\\n Field: {field} | Source: {source_type} to  Target: {target_type} | Rules: {rule_list}\")\n",
    "\n",
    "        try:\n",
    "            expr = col(field)\n",
    "            for rule in rule_list:\n",
    "                handler = get_handler(rule, source_type, target_type)\n",
    "                expr = handler(expr)\n",
    "                print(f\"Applied: {rule}\")\n",
    "            df = df.withColumn(field, expr)\n",
    "            print(f\"Transformed: {field}\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error - Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "    print(\"Field conversion complete.\")\n",
    "    return df"
   ],
   "id": "9b4c2b1bf6436a30",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:11:48.685995Z",
     "start_time": "2025-03-28T06:11:48.680259Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f9513f481ead4d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:11:48.864843Z",
     "start_time": "2025-03-28T06:11:48.837719Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "395f014cdb1ccd9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:11:48.881505Z",
     "start_time": "2025-03-28T06:11:48.872544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, upper, lower, to_date, date_format, regexp_replace, udf, substring\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType, DateType\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def applyFieldConversions(pDF, aAsset, aLogger=None):\n",
    "    df = pDF\n",
    "\n",
    "    def log(msg):\n",
    "        aLogger.info(msg) if aLogger else print(msg)\n",
    "\n",
    "    def cast_to(target_type):\n",
    "        type_map = {\n",
    "            \"String\": StringType(),\n",
    "            \"Int\": IntegerType(),\n",
    "            \"Float\": FloatType(),\n",
    "            \"Date\": DateType()\n",
    "        }\n",
    "        return lambda c: c.cast(type_map[target_type])\n",
    "\n",
    "    def get_handler(rule, source_type, target_type):\n",
    "        rule = rule.strip()\n",
    "\n",
    "        # Handle Left:N (Substring)\n",
    "        if rule.lower().startswith(\"left:\"):\n",
    "            n = int(rule.split(\":\")[1])\n",
    "            return lambda c: substring(c, 1, n)\n",
    "\n",
    "        # Handle Currency Cleanup (???)\n",
    "        if rule == \"???\":\n",
    "            rule = \"Currency:Dollar\"\n",
    "\n",
    "        # Handle Date Parsing or Formatting\n",
    "        if rule.startswith(\"Date:\"):\n",
    "            fmt = rule.split(\"Date:\")[1]\n",
    "\n",
    "            # String to Date\n",
    "            if source_type == \"String\" and target_type == \"Date\":\n",
    "                return lambda c: to_date(c, fmt)\n",
    "\n",
    "            # Date to String - Ensure exact format as defined\n",
    "            elif source_type == \"Date\" and target_type == \"String\":\n",
    "                return lambda c: date_format(c, fmt)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid Date conversion: {source_type} ➡ {target_type}\")\n",
    "\n",
    "        # Static Rule Handlers\n",
    "        handler_map = {\n",
    "            \"Upper\": lambda c: upper(c),\n",
    "            \"Lower\": lambda c: lower(c),\n",
    "            \"Decimal:Comma\": lambda c: regexp_replace(c, \",\", \".\").cast(FloatType()),\n",
    "            \"Currency:Dollar\": lambda c: regexp_replace(regexp_replace(c, \"[$]\", \"\"), \",\", \"\").cast(FloatType()),\n",
    "            \"Standard\": cast_to(target_type)\n",
    "        }\n",
    "\n",
    "        if rule not in handler_map:\n",
    "            raise ValueError(f\"Unsupported rule: {rule}\")\n",
    "\n",
    "        return handler_map[rule]\n",
    "\n",
    "    # Main Logic\n",
    "    log(\"Starting field conversion...\")\n",
    "\n",
    "    for dField in aAsset[\"Fields\"]:\n",
    "        field = dField[\"Target_Field_Name\"]\n",
    "        source_type = dField[\"Source_Data_Type_Code\"]\n",
    "        target_type = dField[\"Target_Data_Type_Code\"]\n",
    "        rule_string = dField.get(\"Conversion_Rule_String\", \"Standard\")\n",
    "        rule_list = [r.strip() for r in rule_string.split(\",\")]\n",
    "\n",
    "        log(f\"Field: {field} | Source: {source_type} ➡ Target: {target_type} | Rules: {rule_list}\")\n",
    "\n",
    "        try:\n",
    "            expr = col(field)\n",
    "            for rule in rule_list:\n",
    "                handler = get_handler(rule, source_type, target_type)\n",
    "                expr = handler(expr)\n",
    "                log(f\"Applied: {rule}\")\n",
    "            df = df.withColumn(field, expr)\n",
    "            log(f\"Transformed: {field}\")\n",
    "        except Exception as e:\n",
    "            log(f\"Error - Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "    log(\"Field conversion complete.\")\n",
    "    return df\n"
   ],
   "id": "f0526121e99361a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:11:48.890816Z",
     "start_time": "2025-03-28T06:11:48.889028Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fca4e341293a5027",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T07:04:53.102804Z",
     "start_time": "2025-03-28T07:04:53.016855Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateConversionTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema Explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"StandardStr\", StringType(), True),\n",
    "    StructField(\"UpperStr\", StringType(), True),\n",
    "    StructField(\"LowerStr\", StringType(), True),\n",
    "    StructField(\"YMDStr\", StringType(), True),\n",
    "    StructField(\"MDYStr\", StringType(), True),\n",
    "    StructField(\"IntStr\", StringType(), True),\n",
    "    StructField(\"FloatStr\", StringType(), True),\n",
    "    StructField(\"CommaStr\", StringType(), True),\n",
    "    StructField(\"DollarStr\", StringType(), True),\n",
    "    StructField(\"IntVal\", IntegerType(), True),\n",
    "    StructField(\"FloatVal\", FloatType(), True),\n",
    "    StructField(\"DateVal\", DateType(), True),\n",
    "    StructField(\"DateAsString\", StringType(), True),\n",
    "    StructField(\"DateTimeStr\", StringType(), True),\n",
    "    StructField(\"DateVal_MMDDYYYY\", StringType(), True),\n",
    "    StructField(\"DateVal_ToInt\", IntegerType(), True),\n",
    "    StructField(\"IntVal_ToFloat\", FloatType(), True),\n",
    "    StructField(\"DateTimeStr_ToDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Test Data with Correct Date Conversion\n",
    "test_data = [\n",
    "    (\"ApPLE\", \"Apple\", \"Apple\", \"2025-05-25\", \"05/25/2025\", \"123\", \"123.45\", \"123,45\", \"$1,567.45\",\n",
    "     2, 123.45, datetime.date(2025, 5, 25), \"2025-05-25\", \"2025-05-25 12:34:56\", \"05/25/2025\", 43056, 2.0, datetime.date(2025, 5, 25))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(test_data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Asset Configuration\n",
    "aAsset = {\n",
    "    \"Fields\": [\n",
    "        # String ➝ String\n",
    "        {\"Target_Field_Name\": \"StandardStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"UpperStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Upper\"},\n",
    "        {\"Target_Field_Name\": \"LowerStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Lower\"},\n",
    "\n",
    "        # String ➝ Date\n",
    "        {\"Target_Field_Name\": \"YMDStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"MDYStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"},\n",
    "\n",
    "        # Date ➝ String / Int\n",
    "        {\"Target_Field_Name\": \"DateAsString\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"DateVal_MMDDYYYY\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"},\n",
    "        {\"Target_Field_Name\": \"DateVal_ToInt\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"Int\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "\n",
    "        # String ➝ Int / Float\n",
    "        {\"Target_Field_Name\": \"IntStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Int\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"FloatStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"CommaStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Decimal:Comma\"},\n",
    "        {\"Target_Field_Name\": \"DollarStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"???\"},\n",
    "\n",
    "        # Int ➝ Int / Float\n",
    "        {\"Target_Field_Name\": \"IntVal\", \"Source_Data_Type_Code\": \"Int\", \"Target_Data_Type_Code\": \"Int\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"IntVal_ToFloat\", \"Source_Data_Type_Code\": \"Int\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "\n",
    "        # Float ➝ Float\n",
    "        {\"Target_Field_Name\": \"FloatVal\", \"Source_Data_Type_Code\": \"Float\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "\n",
    "        # String ➝ Date using Left + Format\n",
    "        {\"Target_Field_Name\": \"DateTimeStr_ToDate\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Left:10,Date:yyyy-MM-dd\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"✅ Data and Asset Configuration Ready for Testing!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal   |DateAsString|DateTimeStr        |DateVal_MMDDYYYY|DateVal_ToInt|IntVal_ToFloat|DateTimeStr_ToDate|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|ApPLE      |Apple   |Apple   |2025-05-25|05/25/2025|123   |123.45  |123,45  |$1,567.45|2     |123.45  |2025-05-25|2025-05-25  |2025-05-25 12:34:56|05/25/2025      |43056        |2.0           |2025-05-25        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "\n",
      "✅ Data and Asset Configuration Ready for Testing!\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:51:15.894367Z",
     "start_time": "2025-03-28T06:51:15.834536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.createDataFrame(test_data, schema=schema)\n",
    "df.show(truncate=False)"
   ],
   "id": "2dcca91c1d67202d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal   |DateAsString|DateTimeStr        |DateVal_MMDDYYYY|DateVal_ToInt|IntVal_ToFloat|DateTimeStr_ToDate|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|ApPLE      |Apple   |Apple   |2025-05-25|05/25/2025|123   |123.45  |123,45  |$1,567.45|2     |123.45  |2025-05-25|2025-05-25  |2025-05-25 12:34:56|05/25/2025      |43056        |2.0           |2025-05-25        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:51:17.489802Z",
     "start_time": "2025-03-28T06:51:17.421338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply Conversion\n",
    "converted_df = applyFieldConversions(df, aAsset)"
   ],
   "id": "3cf8560d6a395db3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting field conversion...\n",
      "\n",
      " Field: StandardStr | Source: String to  Target: String | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: StandardStr\n",
      "\n",
      " Field: UpperStr | Source: String to  Target: String | Rules: ['Upper']\n",
      "Applied: Upper\n",
      "Transformed: UpperStr\n",
      "\n",
      " Field: LowerStr | Source: String to  Target: String | Rules: ['Lower']\n",
      "Applied: Lower\n",
      "Transformed: LowerStr\n",
      "\n",
      " Field: YMDStr | Source: String to  Target: Date | Rules: ['Date:yyyy-MM-dd']\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: YMDStr\n",
      "\n",
      " Field: MDYStr | Source: String to  Target: Date | Rules: ['Date:MM/dd/yyyy']\n",
      "Applied: Date:MM/dd/yyyy\n",
      "Transformed: MDYStr\n",
      "\n",
      " Field: DateAsString | Source: Date to  Target: String | Rules: ['Date:yyyy-MM-dd']\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: DateAsString\n",
      "\n",
      " Field: DateVal_MMDDYYYY | Source: Date to  Target: String | Rules: ['Date:MM/dd/yyyy']\n",
      "Applied: Date:MM/dd/yyyy\n",
      "Transformed: DateVal_MMDDYYYY\n",
      "\n",
      " Field: DateVal_ToInt | Source: Date to  Target: Int | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: DateVal_ToInt\n",
      "\n",
      " Field: IntStr | Source: String to  Target: Int | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: IntStr\n",
      "\n",
      " Field: FloatStr | Source: String to  Target: Float | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: FloatStr\n",
      "\n",
      " Field: CommaStr | Source: String to  Target: Float | Rules: ['Decimal:Comma']\n",
      "Applied: Decimal:Comma\n",
      "Transformed: CommaStr\n",
      "\n",
      " Field: DollarStr | Source: String to  Target: Float | Rules: ['???']\n",
      "Applied: ???\n",
      "Transformed: DollarStr\n",
      "\n",
      " Field: IntVal | Source: Int to  Target: Int | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: IntVal\n",
      "\n",
      " Field: IntVal_ToFloat | Source: Int to  Target: Float | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: IntVal_ToFloat\n",
      "\n",
      " Field: FloatVal | Source: Float to  Target: Float | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: FloatVal\n",
      "\n",
      " Field: DateTimeStr_ToDate | Source: String to  Target: Date | Rules: ['Left:10', 'Date:yyyy-MM-dd']\n",
      "Applied: Left:10\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: DateTimeStr_ToDate\n",
      "Field conversion complete.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:51:24.311802Z",
     "start_time": "2025-03-28T06:51:24.219107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show Result\n",
    "converted_df.show(truncate=False)"
   ],
   "id": "bb77f8e7955a12ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal   |DateAsString|DateTimeStr        |DateVal_MMDDYYYY|DateVal_ToInt|IntVal_ToFloat|DateTimeStr_ToDate|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "|ApPLE      |APPLE   |apple   |2025-05-25|2025-05-25|123   |123.45  |123.45  |1567.45  |2     |123.45  |2025-05-25|2025-05-25  |2025-05-25 12:34:56|NULL            |43056        |2.0           |2025-05-25        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+----------------+-------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T06:52:42.061513Z",
     "start_time": "2025-03-28T06:52:42.056989Z"
    }
   },
   "cell_type": "code",
   "source": "converted_df.printSchema()",
   "id": "eba203fb44a41cbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- StandardStr: string (nullable = true)\n",
      " |-- UpperStr: string (nullable = true)\n",
      " |-- LowerStr: string (nullable = true)\n",
      " |-- YMDStr: date (nullable = true)\n",
      " |-- MDYStr: date (nullable = true)\n",
      " |-- IntStr: integer (nullable = true)\n",
      " |-- FloatStr: float (nullable = true)\n",
      " |-- CommaStr: float (nullable = true)\n",
      " |-- DollarStr: float (nullable = true)\n",
      " |-- IntVal: integer (nullable = true)\n",
      " |-- FloatVal: float (nullable = true)\n",
      " |-- DateVal: date (nullable = true)\n",
      " |-- DateAsString: string (nullable = true)\n",
      " |-- DateTimeStr: string (nullable = true)\n",
      " |-- DateVal_MMDDYYYY: string (nullable = true)\n",
      " |-- DateVal_ToInt: integer (nullable = true)\n",
      " |-- IntVal_ToFloat: float (nullable = true)\n",
      " |-- DateTimeStr_ToDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "32f7d555b141b56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:59:24.657582Z",
     "start_time": "2025-03-28T07:59:24.589152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateConversionTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema Explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"DateVal\", DateType(), True),\n",
    "    StructField(\"DateAsString\", DateType(), True),\n",
    "    StructField(\"DateVal_MMDDYYYY\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Test Data with Correct Date Conversion\n",
    "test_data = [\n",
    "    (datetime.date(2025, 5, 25), datetime.date(2025, 5, 25), datetime.date(2025, 5, 25))\n",
    "]\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"Int_To_Float\", IntegerType(), True),\n",
    "#     StructField(\"String_To_Float\", StringType(), True),\n",
    "#     StructField(\"String_To_Float_Decimal\", StringType(), True),\n",
    "#     StructField(\"Float_To_Float\", FloatType(), True)\n",
    "# ])\n",
    "\n",
    "# Test Data with Correct Date Conversion\n",
    "# test_data = [\n",
    "#     (1, \"12.5\", \"1,345\", 12.5)\n",
    "# ]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(test_data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Asset Configuration\n",
    "aAsset = {\n",
    "    \"Fields\": [\n",
    "        # Date ➝ String\n",
    "        {\"Target_Field_Name\": \"DateVal\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"DateAsString\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"DateVal_MMDDYYYY\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"}\n",
    "\n",
    "        # Float Check\n",
    "        # {\"Target_Field_Name\": \"Int_To_Float\", \"Source_Data_Type_Code\": \"Int\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        # {\"Target_Field_Name\": \"String_To_Float\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        # {\"Target_Field_Name\": \"String_To_Float_Decimal\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Decimal:Comma\"},\n",
    "        # {\"Target_Field_Name\": \"Float_To_Float\", \"Source_Data_Type_Code\": \"Float\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"}\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"✅ Data and Asset Configuration Ready for Testing!\")\n"
   ],
   "id": "e0f73495e2c55708",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+\n",
      "|DateVal   |DateAsString|DateVal_MMDDYYYY|\n",
      "+----------+------------+----------------+\n",
      "|2025-05-25|2025-05-25  |2025-05-25      |\n",
      "+----------+------------+----------------+\n",
      "\n",
      "✅ Data and Asset Configuration Ready for Testing!\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:59:32.414063Z",
     "start_time": "2025-03-28T07:59:32.357984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "converted_df = applyFieldConversions(df, aAsset)\n",
    "converted_df.show(truncate=False)"
   ],
   "id": "71754e05e74111c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting field conversion...\n",
      "\n",
      " Field: DateVal | Source: Date to  Target: Date | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: DateVal\n",
      "\n",
      " Field: DateAsString | Source: Date to  Target: String | Rules: ['Date:yyyy-MM-dd']\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: DateAsString\n",
      "\n",
      " Field: DateVal_MMDDYYYY | Source: Date to  Target: String | Rules: ['Date:MM/dd/yyyy']\n",
      "Applied: Date:MM/dd/yyyy\n",
      "Transformed: DateVal_MMDDYYYY\n",
      "Field conversion complete.\n",
      "+----------+------------+----------------+\n",
      "|DateVal   |DateAsString|DateVal_MMDDYYYY|\n",
      "+----------+------------+----------------+\n",
      "|2025-05-25|2025-05-25  |05/25/2025      |\n",
      "+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T07:59:39.781082Z",
     "start_time": "2025-03-28T07:59:39.777233Z"
    }
   },
   "cell_type": "code",
   "source": "converted_df.printSchema()",
   "id": "6b982003acca1735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateVal: date (nullable = true)\n",
      " |-- DateAsString: string (nullable = true)\n",
      " |-- DateVal_MMDDYYYY: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T07:30:41.779625Z",
     "start_time": "2025-04-02T07:30:36.297500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, to_date, date_format, upper, lower, regexp_replace, substring, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType, IntegerType\n",
    "import datetime\n",
    "\n",
    "class DIFFieldConversion:\n",
    "    def __init__(self, df, asset_config):\n",
    "        self.df = df\n",
    "        self.asset_config = asset_config\n",
    "\n",
    "    def convertStringToDate(self, df, field, fmt):\n",
    "        return df.withColumn(field, to_date(col(field), fmt))\n",
    "\n",
    "    def convertDateToString(self, df, field, fmt):\n",
    "        return df.withColumn(field, date_format(col(field), fmt))\n",
    "\n",
    "    def convertStringToString(self, df, field, fmt):\n",
    "        return df.withColumn(field, date_format(to_date(col(field), 'yyyy-MM-dd'), fmt))\n",
    "\n",
    "    def convertToUpper(self, df, field):\n",
    "        return df.withColumn(field, upper(col(field)))\n",
    "\n",
    "    def convertToLower(self, df, field):\n",
    "        return df.withColumn(field, lower(col(field)))\n",
    "\n",
    "    def convertDecimalCommaToFloat(self, df, field):\n",
    "        # Handles both 1.234,56 and 1,234.56 or 1,100,345.56 formats\n",
    "        clean_col = regexp_replace(regexp_replace(col(field), \"[.]\", \"\"), \",\", \".\")\n",
    "        return df.withColumn(field, clean_col.cast(FloatType()))\n",
    "\n",
    "    def convertCurrencyDollarToFloat(self, df, field):\n",
    "        return df.withColumn(field, regexp_replace(regexp_replace(col(field), \"[$]\", \"\"), \",\", \"\").cast(FloatType()))\n",
    "\n",
    "    def convertDateToInt(self, df, field):\n",
    "        def date_to_days(d):\n",
    "            if d:\n",
    "                base = datetime.date(1900, 1, 1)\n",
    "                if isinstance(d, datetime.datetime):\n",
    "                    d = d.date()\n",
    "                return (d - base).days if isinstance(d, datetime.date) else None\n",
    "            return None\n",
    "        return df.withColumn(field, udf(date_to_days, IntegerType())(col(field)))\n",
    "\n",
    "    def convertStringToInt(self, df, field):\n",
    "        return df.withColumn(field, col(field).cast(IntegerType()))\n",
    "\n",
    "    def convertStringToFloat(self, df, field):\n",
    "        return df.withColumn(field, col(field).cast(FloatType()))\n",
    "\n",
    "    def convertIntToFloat(self, df, field):\n",
    "        return df.withColumn(field, col(field).cast(FloatType()))\n",
    "\n",
    "    def applyConversions(self):\n",
    "        df = self.df\n",
    "        print(\"Starting field conversion...\")\n",
    "\n",
    "        for field_config in self.asset_config[\"Fields\"]:\n",
    "            field = field_config[\"Target_Field_Name\"]\n",
    "            sourceType = field_config[\"Source_Data_Type_Code\"]\n",
    "            targetType = field_config[\"Target_Data_Type_Code\"]\n",
    "            rule_string = field_config.get(\"Conversion_Rule_String\", \"Standard\")\n",
    "            rule_list = [r.strip() for r in rule_string.split(\",\")]\n",
    "\n",
    "            print(f\"\\n Field: {field} | Source: {sourceType} to  Target: {targetType} | Rules: {rule_list}\")\n",
    "\n",
    "            try:\n",
    "                for rule in rule_list:\n",
    "                    if rule.startswith(\"Date:\"):\n",
    "                        fmt = rule.split(\"Date:\")[1]\n",
    "                        if sourceType == \"String\" and targetType == \"Date\":\n",
    "                            df = self.convertStringToDate(df, field, fmt)\n",
    "                        elif sourceType == \"Date\" and targetType == \"String\":\n",
    "                            df = self.convertDateToString(df, field, fmt)\n",
    "                        elif sourceType == \"String\" and targetType == \"String\":\n",
    "                            df = self.convertStringToString(df, field, fmt)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Invalid Date conversion: {sourceType} to {targetType}\")\n",
    "                        print(f\"Applied: {rule}\")\n",
    "\n",
    "                    elif rule == \"Upper\":\n",
    "                        df = self.convertToUpper(df, field)\n",
    "                        print(f\"Applied: {rule}\")\n",
    "\n",
    "                    elif rule == \"Lower\":\n",
    "                        df = self.convertToLower(df, field)\n",
    "                        print(f\"Applied: {rule}\")\n",
    "\n",
    "                    elif rule == \"Decimal:Comma\":\n",
    "                        df = self.convertDecimalCommaToFloat(df, field)\n",
    "                        print(f\"Applied: {rule}\")\n",
    "\n",
    "                    elif rule == \"???\":\n",
    "                        df = self.convertCurrencyDollarToFloat(df, field)\n",
    "                        print(f\"Applied: {rule} (Dollar Currency)\")\n",
    "\n",
    "                    elif rule == \"Standard\":\n",
    "                        if sourceType == \"String\" and targetType == \"Int\":\n",
    "                            df = self.convertStringToInt(df, field)\n",
    "                        elif sourceType == \"String\" and targetType == \"Float\":\n",
    "                            df = self.convertStringToFloat(df, field)\n",
    "                        elif sourceType == \"Int\" and targetType == \"Float\":\n",
    "                            df = self.convertIntToFloat(df, field)\n",
    "                        elif sourceType == \"Date\" and targetType == \"Int\":\n",
    "                            df = self.convertDateToInt(df, field)\n",
    "                        print(f\"Applied: {rule}\")\n",
    "\n",
    "                print(f\"Transformed: {field}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error - Field: {field} | Reason: {str(e)}\")\n",
    "\n",
    "        print(\"Field conversion complete.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DateConversionTest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema Explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"StandardStr\", StringType(), True),\n",
    "    StructField(\"UpperStr\", StringType(), True),\n",
    "    StructField(\"LowerStr\", StringType(), True),\n",
    "    StructField(\"YMDStr\", StringType(), True),\n",
    "    StructField(\"MDYStr\", StringType(), True),\n",
    "    StructField(\"IntStr\", StringType(), True),\n",
    "    StructField(\"FloatStr\", StringType(), True),\n",
    "    StructField(\"CommaStr\", StringType(), True),\n",
    "    StructField(\"DollarStr\", StringType(), True),\n",
    "    StructField(\"IntVal\", IntegerType(), True),\n",
    "    StructField(\"FloatVal\", FloatType(), True),\n",
    "    StructField(\"DateVal\", DateType(), True),\n",
    "    StructField(\"DateAsString\", StringType(), True),\n",
    "    StructField(\"DateTimeStr\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample Data Row\n",
    "test_data = [\n",
    "    (\"ApPLE\", \"Apple\", \"Apple\", \"2025-05-25\", \"05/25/2025\", \"123\", \"123.45\", \"123,45\", \"$123.45\", 2, 123.45, datetime.date(2025, 5, 25), \"2025-05-25\", \"2025-05-25 12:34:56\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(test_data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Asset Configuration\n",
    "aAsset = {\n",
    "    \"Fields\": [\n",
    "        {\"Target_Field_Name\": \"StandardStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"UpperStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Upper\"},\n",
    "        {\"Target_Field_Name\": \"LowerStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Lower\"},\n",
    "        {\"Target_Field_Name\": \"YMDStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"MDYStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Date\", \"Conversion_Rule_String\": \"Date:MM/dd/yyyy\"},\n",
    "        {\"Target_Field_Name\": \"DateAsString\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"String\", \"Conversion_Rule_String\": \"Date:yyyy-MM-dd\"},\n",
    "        {\"Target_Field_Name\": \"IntStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Int\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"FloatStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"CommaStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Decimal:Comma\"},\n",
    "        {\"Target_Field_Name\": \"DollarStr\", \"Source_Data_Type_Code\": \"String\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"???\"},\n",
    "        {\"Target_Field_Name\": \"IntVal\", \"Source_Data_Type_Code\": \"Int\", \"Target_Data_Type_Code\": \"Float\", \"Conversion_Rule_String\": \"Standard\"},\n",
    "        {\"Target_Field_Name\": \"DateVal\", \"Source_Data_Type_Code\": \"Date\", \"Target_Data_Type_Code\": \"Int\", \"Conversion_Rule_String\": \"Standard\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Apply conversion\n",
    "converter = DIFFieldConversion(df, aAsset)\n",
    "converted_df = converter.applyConversions()\n",
    "converted_df.show(truncate=False)\n",
    "\n",
    "print(\"✅ Field conversion completed!\")\n"
   ],
   "id": "ded15cd07716d2a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/02 13:00:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal   |DateAsString|DateTimeStr        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+\n",
      "|ApPLE      |Apple   |Apple   |2025-05-25|05/25/2025|123   |123.45  |123,45  |$123.45  |2     |123.45  |2025-05-25|2025-05-25  |2025-05-25 12:34:56|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+----------+------------+-------------------+\n",
      "\n",
      "Starting field conversion...\n",
      "\n",
      " Field: StandardStr | Source: String to  Target: String | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: StandardStr\n",
      "\n",
      " Field: UpperStr | Source: String to  Target: String | Rules: ['Upper']\n",
      "Applied: Upper\n",
      "Transformed: UpperStr\n",
      "\n",
      " Field: LowerStr | Source: String to  Target: String | Rules: ['Lower']\n",
      "Applied: Lower\n",
      "Transformed: LowerStr\n",
      "\n",
      " Field: YMDStr | Source: String to  Target: Date | Rules: ['Date:yyyy-MM-dd']\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: YMDStr\n",
      "\n",
      " Field: MDYStr | Source: String to  Target: Date | Rules: ['Date:MM/dd/yyyy']\n",
      "Applied: Date:MM/dd/yyyy\n",
      "Transformed: MDYStr\n",
      "\n",
      " Field: DateAsString | Source: Date to  Target: String | Rules: ['Date:yyyy-MM-dd']\n",
      "Applied: Date:yyyy-MM-dd\n",
      "Transformed: DateAsString\n",
      "\n",
      " Field: IntStr | Source: String to  Target: Int | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: IntStr\n",
      "\n",
      " Field: FloatStr | Source: String to  Target: Float | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: FloatStr\n",
      "\n",
      " Field: CommaStr | Source: String to  Target: Float | Rules: ['Decimal:Comma']\n",
      "Applied: Decimal:Comma\n",
      "Transformed: CommaStr\n",
      "\n",
      " Field: DollarStr | Source: String to  Target: Float | Rules: ['???']\n",
      "Applied: ??? (Dollar Currency)\n",
      "Transformed: DollarStr\n",
      "\n",
      " Field: IntVal | Source: Int to  Target: Float | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: IntVal\n",
      "\n",
      " Field: DateVal | Source: Date to  Target: Int | Rules: ['Standard']\n",
      "Applied: Standard\n",
      "Transformed: DateVal\n",
      "Field conversion complete.\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal|DateAsString|DateTimeStr        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "|ApPLE      |APPLE   |apple   |2025-05-25|2025-05-25|123   |123.45  |123.45  |123.45   |2.0   |123.45  |45800  |2025-05-25  |2025-05-25 12:34:56|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "\n",
      "✅ Field conversion completed!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T07:31:10.636579Z",
     "start_time": "2025-04-02T07:31:10.497991Z"
    }
   },
   "cell_type": "code",
   "source": "converted_df.show(truncate=False)",
   "id": "4ed2d1c114391576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "|StandardStr|UpperStr|LowerStr|YMDStr    |MDYStr    |IntStr|FloatStr|CommaStr|DollarStr|IntVal|FloatVal|DateVal|DateAsString|DateTimeStr        |\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "|ApPLE      |APPLE   |apple   |2025-05-25|2025-05-25|123   |123.45  |123.45  |123.45   |2.0   |123.45  |45800  |2025-05-25  |2025-05-25 12:34:56|\n",
      "+-----------+--------+--------+----------+----------+------+--------+--------+---------+------+--------+-------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T07:55:27.067407Z",
     "start_time": "2025-04-04T07:55:27.061046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compress(chars):\n",
    "\n",
    "        s = \"\"\n",
    "        prev_char = None\n",
    "        for char in chars:\n",
    "            if prev_char is None:\n",
    "                prev_char = char\n",
    "                cnt = 1\n",
    "                continue\n",
    "\n",
    "            if char == prev_char:\n",
    "                cnt += 1\n",
    "            else:\n",
    "                if cnt == 1:\n",
    "                    s += prev_char\n",
    "                else:\n",
    "                    s += prev_char + str(cnt)\n",
    "\n",
    "                prev_char = char\n",
    "                cnt = 1\n",
    "\n",
    "        if cnt == 1:\n",
    "            s += prev_char\n",
    "        else:\n",
    "            s += prev_char + str(cnt)\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            chars[i] = s[i]\n",
    "\n",
    "\n",
    "        print(s, chars)\n",
    "        return len(s)\n",
    "\n",
    "chars = [\"a\",\"a\",\"b\",\"c\",\"c\",\"c\"]\n",
    "compress(chars)"
   ],
   "id": "46f7447f307356d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2bc3 ['a', '2', 'b', 'c', '3', 'c']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
