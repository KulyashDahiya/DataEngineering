{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T10:39:02.579480Z",
     "start_time": "2025-04-17T10:39:02.468495Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_timestamp\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DIFProcessedHelperTest\").getOrCreate()\n",
    "\n",
    "# Mock the DIFLogger class to simply print logs\n",
    "class DIFLogger:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def info(self, message):\n",
    "        print(f\"INFO: {message}\")\n",
    "\n",
    "    def debug(self, message):\n",
    "        print(f\"DEBUG: {message}\")\n",
    "\n",
    "    def error(self, message):\n",
    "        print(f\"ERROR: {message}\")\n",
    "\n",
    "    def warn(self, message):\n",
    "        print(f\"WARNING: {message}\")\n",
    "\n",
    "\n",
    "# Mock Asset with Fields and Schema\n",
    "mockAsset = {\n",
    "    \"Processed_Table_Name\": \"mock_processed_table\",\n",
    "    \"Load_Type_Code\": \"T1-All-Replace\",  # Example load type\n",
    "    \"BusinessDate\": \"2025-04-17\",  # Example business date\n",
    "    \"IngestFile\": \"file1.csv\",  # Example source file name\n",
    "    \"Fields\": [\n",
    "        {\"Field_Sequence_Number\": 1, \"Target_Field_Name\": \"SalesDate\", \"IsKey_Indicator\": \"Y\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Field_Sequence_Number\": 2, \"Target_Field_Name\": \"SKU\", \"IsKey_Indicator\": \"Y\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Field_Sequence_Number\": 3, \"Target_Field_Name\": \"Units\", \"IsKey_Indicator\": \"N\", \"Target_Data_Type_Code\": \"INTEGER\"},\n",
    "        {\"Field_Sequence_Number\": 4, \"Target_Field_Name\": \"DIFSourceFile\", \"IsKey_Indicator\": \"N\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Field_Sequence_Number\": 5, \"Target_Field_Name\": \"BusinessDate\", \"IsKey_Indicator\": \"N\", \"Target_Data_Type_Code\": \"STRING\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the target DataFrame (dfTarget) to simulate the Processed Table (for two days of data)\n",
    "data_target = [\n",
    "    (\"2024-01-12\", \"A\", 100, \"file1.csv\", \"2024-01-12\"),\n",
    "    (\"2024-01-12\", \"B\", 200, \"file1.csv\", \"2024-01-12\"),\n",
    "    (\"2024-01-13\", \"A\", 150, \"file2.csv\", \"2024-01-13\"),\n",
    "    (\"2024-01-13\", \"B\", 250, \"file2.csv\", \"2024-01-13\")\n",
    "]\n",
    "\n",
    "# Define schema based on mockAsset[\"Fields\"]\n",
    "schema_target = StructType([\n",
    "    StructField(\"SalesDate\", StringType(), True),\n",
    "    StructField(\"SKU\", StringType(), True),\n",
    "    StructField(\"Units\", IntegerType(), True),\n",
    "    StructField(\"DIFSourceFile\", StringType(), True),\n",
    "    StructField(\"BusinessDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "dfTarget = spark.createDataFrame(data_target, schema_target)\n",
    "\n",
    "# Create the incoming DataFrame (dfIncoming) to simulate the new incoming data (for two days of data)\n",
    "data_incoming = [\n",
    "    (\"2024-01-12\", \"A\", 110, \"file1.csv\", \"2024-01-12\"),\n",
    "    (\"2024-01-13\", \"B\", 220, \"file1.csv\", \"2024-01-13\"),\n",
    "    (\"2024-01-14\", \"C\", 300, \"file2.csv\", \"2024-01-14\")\n",
    "]\n",
    "\n",
    "# Define schema for incoming data\n",
    "schema_incoming = StructType([\n",
    "    StructField(\"SalesDate\", StringType(), True),\n",
    "    StructField(\"SKU\", StringType(), True),\n",
    "    StructField(\"Units\", IntegerType(), True),\n",
    "    StructField(\"DIFSourceFile\", StringType(), True),\n",
    "    StructField(\"BusinessDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "dfIncoming = spark.createDataFrame(data_incoming, schema_incoming)\n",
    "\n",
    "# Define the DIFProcessedTableHelperTest class (for testing)\n",
    "class DIFProcessedTableHelperTest:\n",
    "    def __init__(self, pEnvConfig, pAssetGroupConfig, pLogger, pSpark, pAsset):\n",
    "        self.aEnvConfig = pEnvConfig\n",
    "        self.aAssetGroupConfig = pAssetGroupConfig\n",
    "        self.aLogger = pLogger\n",
    "        self.aSpark = pSpark\n",
    "        self.aAsset = pAsset\n",
    "        self.aInstructions = {\"Description\": \"T1-All-Replace\", \"Load_Type_Code\": \"T1-All-Replace\", \"FromToColumns\": True, \"Delete\": \"All\"}\n",
    "\n",
    "    def applyTempRecordsToTTable(self, pTimeStamp, dfIncoming, dfTarget):\n",
    "        try:\n",
    "            self.aLogger.info(\"applyTempRecordsToTTable.Start\")\n",
    "\n",
    "            # Get the main processed table (T Table)\n",
    "            mainTable = self.aAsset[\"Processed_Table_Name\"]\n",
    "            self.aLogger.info(\"Processing table: \" + mainTable)\n",
    "\n",
    "            # Add CDC_LOAD_CODE and LOAD_TS to incoming data before processing\n",
    "            dfIncoming = dfIncoming.withColumn(\"CDC_LOAD_CODE\", lit(\"I\"))\\\n",
    "                .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "            # Handle From/To and Updates (if applicable)\n",
    "            if self.aInstructions[\"FromToColumns\"]:\n",
    "                # Update the records where SalesDate and SKU match\n",
    "                dfUpdates = dfIncoming.join(dfTarget, on=[\"SKU\", \"SalesDate\"], how=\"inner\")\\\n",
    "                    .filter(dfTarget[\"Units\"] != dfIncoming[\"Units\"])\n",
    "\n",
    "                dfUpdates = dfUpdates.withColumn(\"CDC_LOAD_CODE\", lit(\"U\"))\\\n",
    "                    .withColumn(\"SalesDate\", lit(self.aAsset[\"BusinessDate\"]))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "                # Remove old data and add the updated data\n",
    "                dfTarget = dfTarget.subtract(dfUpdates)  # Remove old data that's being updated\n",
    "                dfTarget = dfTarget.union(dfUpdates)  # Add updated data\n",
    "\n",
    "            # Handle From/To + Deletes (if applicable)\n",
    "            if self.aInstructions[\"FromToColumns\"]:\n",
    "                # Mark records for deletion where CDC_LOAD_CODE = 'I' and END_TS = '2999-12-31'\n",
    "                dfDeletes = dfIncoming.join(dfTarget, on=[\"SKU\", \"SalesDate\"], how=\"left_anti\")\n",
    "\n",
    "                dfDeletes = dfDeletes.withColumn(\"CDC_LOAD_CODE\", lit(\"D\"))\\\n",
    "                    .withColumn(\"SalesDate\", lit(self.aAsset[\"BusinessDate\"]))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "                # Remove deleted records from T Table\n",
    "                dfTarget = dfTarget.subtract(dfDeletes)  # Remove records to be deleted\n",
    "                dfTarget = dfTarget.union(dfDeletes)  # Add deleted records\n",
    "\n",
    "            # Handle Missing Keys: Mark as Deleted (if applicable)\n",
    "            if self.aInstructions[\"Delete\"] == \"MissingKeys\":\n",
    "                missingKeys = dfTarget.join(dfIncoming, on=[\"SKU\"], how=\"left_anti\")\n",
    "\n",
    "                missingKeys = missingKeys.withColumn(\"CDC_LOAD_CODE\", lit(\"D\"))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\\\n",
    "                    .withColumn(\"SalesDate\", lit(self.aAsset[\"BusinessDate\"]))\n",
    "\n",
    "                # Update T Table for missing keys (soft delete)\n",
    "                dfTarget = dfTarget.subtract(missingKeys)  # Remove missing keys\n",
    "                dfTarget = dfTarget.union(missingKeys)  # Add missing keys as deleted\n",
    "\n",
    "            # Finally, add the new data to the T Table (Insert)\n",
    "            dfInserts = dfIncoming.withColumn(\"CDC_LOAD_CODE\", lit(\"I\"))\\\n",
    "                .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "            # Add the new data to T Table\n",
    "            dfTarget = dfTarget.union(dfInserts)\n",
    "\n",
    "            self.aLogger.info(\"applyTempRecordsToTTable.End\")\n",
    "            return dfTarget\n",
    "\n",
    "        except Exception as ex:\n",
    "            self.aLogger.error(\"applyTempRecordsToTTable.Error: \" + str(ex))\n",
    "            raise Exception(str(ex))\n",
    "\n",
    "\n",
    "# Initialize the mock logger\n",
    "mockLogger = DIFLogger()\n",
    "\n",
    "# Instantiate the helper class with mock configurations\n",
    "helper = DIFProcessedTableHelperTest(mockEnvConfig, mockAssetGroupConfig, mockLogger, spark, mockAsset)\n",
    "\n",
    "# Run the test example\n",
    "processedFinal = helper.applyTempRecordsToTTable(\"2024-01-15 00:00:00\", dfIncoming, dfTarget)\n",
    "\n",
    "# Show the processed result\n",
    "processedFinal.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: applyTempRecordsToTTable.Start\n",
      "INFO: Processing table: mock_processed_table\n",
      "ERROR: applyTempRecordsToTTable.Error: [NUM_COLUMNS_MISMATCH] EXCEPT can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 10 columns.;\n",
      "'Except false\n",
      ":- LogicalRDD [SalesDate#97, SKU#98, Units#99, DIFSourceFile#100, BusinessDate#101], false\n",
      "+- Project [SKU#108, SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#164, Units#177, DIFSourceFile#178, BusinessDate#179]\n",
      "   +- Project [SKU#108, 2025-04-17 AS SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n",
      "      +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, U AS CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n",
      "         +- Filter NOT (Units#177 = Units#109)\n",
      "            +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n",
      "               +- Join Inner, ((SKU#108 = SKU#176) AND (SalesDate#107 = SalesDate#175))\n",
      "                  :- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#124]\n",
      "                  :  +- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, I AS CDC_LOAD_CODE#117]\n",
      "                  :     +- LogicalRDD [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111], false\n",
      "                  +- LogicalRDD [SalesDate#175, SKU#176, Units#177, DIFSourceFile#178, BusinessDate#179], false\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[NUM_COLUMNS_MISMATCH] EXCEPT can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 10 columns.;\n'Except false\n:- LogicalRDD [SalesDate#97, SKU#98, Units#99, DIFSourceFile#100, BusinessDate#101], false\n+- Project [SKU#108, SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#164, Units#177, DIFSourceFile#178, BusinessDate#179]\n   +- Project [SKU#108, 2025-04-17 AS SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n      +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, U AS CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n         +- Filter NOT (Units#177 = Units#109)\n            +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n               +- Join Inner, ((SKU#108 = SKU#176) AND (SalesDate#107 = SalesDate#175))\n                  :- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#124]\n                  :  +- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, I AS CDC_LOAD_CODE#117]\n                  :     +- LogicalRDD [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111], false\n                  +- LogicalRDD [SalesDate#175, SKU#176, Units#177, DIFSourceFile#178, BusinessDate#179], false\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 110\u001B[0m, in \u001B[0;36mDIFProcessedTableHelperTest.applyTempRecordsToTTable\u001B[0;34m(self, pTimeStamp, dfIncoming, dfTarget)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;66;03m# Remove old data and add the updated data\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m dfTarget \u001B[38;5;241m=\u001B[39m \u001B[43mdfTarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubtract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdfUpdates\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Remove old data that's being updated\u001B[39;00m\n\u001B[1;32m    111\u001B[0m dfTarget \u001B[38;5;241m=\u001B[39m dfTarget\u001B[38;5;241m.\u001B[39munion(dfUpdates)  \u001B[38;5;66;03m# Add updated data\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/dataframe.py:4155\u001B[0m, in \u001B[0;36mDataFrame.subtract\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   4122\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m   4123\u001B[0m \u001B[38;5;124;03mbut not in another :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   4124\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4153\u001B[0m \u001B[38;5;124;03m+---+---+\u001B[39;00m\n\u001B[1;32m   4154\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m-> 4155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexcept\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/errors/exceptions/captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] EXCEPT can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 10 columns.;\n'Except false\n:- LogicalRDD [SalesDate#97, SKU#98, Units#99, DIFSourceFile#100, BusinessDate#101], false\n+- Project [SKU#108, SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#164, Units#177, DIFSourceFile#178, BusinessDate#179]\n   +- Project [SKU#108, 2025-04-17 AS SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n      +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, U AS CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n         +- Filter NOT (Units#177 = Units#109)\n            +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n               +- Join Inner, ((SKU#108 = SKU#176) AND (SalesDate#107 = SalesDate#175))\n                  :- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#124]\n                  :  +- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, I AS CDC_LOAD_CODE#117]\n                  :     +- LogicalRDD [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111], false\n                  +- LogicalRDD [SalesDate#175, SKU#176, Units#177, DIFSourceFile#178, BusinessDate#179], false\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 160\u001B[0m\n\u001B[1;32m    157\u001B[0m helper \u001B[38;5;241m=\u001B[39m DIFProcessedTableHelperTest(mockEnvConfig, mockAssetGroupConfig, mockLogger, spark, mockAsset)\n\u001B[1;32m    159\u001B[0m \u001B[38;5;66;03m# Run the test example\u001B[39;00m\n\u001B[0;32m--> 160\u001B[0m processedFinal \u001B[38;5;241m=\u001B[39m \u001B[43mhelper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplyTempRecordsToTTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m2024-01-15 00:00:00\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdfIncoming\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdfTarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;66;03m# Show the processed result\u001B[39;00m\n\u001B[1;32m    163\u001B[0m processedFinal\u001B[38;5;241m.\u001B[39mshow()\n",
      "Cell \u001B[0;32mIn[4], line 150\u001B[0m, in \u001B[0;36mDIFProcessedTableHelperTest.applyTempRecordsToTTable\u001B[0;34m(self, pTimeStamp, dfIncoming, dfTarget)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maLogger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplyTempRecordsToTTable.Error: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(ex))\n\u001B[0;32m--> 150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;28mstr\u001B[39m(ex))\n",
      "\u001B[0;31mException\u001B[0m: [NUM_COLUMNS_MISMATCH] EXCEPT can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 10 columns.;\n'Except false\n:- LogicalRDD [SalesDate#97, SKU#98, Units#99, DIFSourceFile#100, BusinessDate#101], false\n+- Project [SKU#108, SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#164, Units#177, DIFSourceFile#178, BusinessDate#179]\n   +- Project [SKU#108, 2025-04-17 AS SalesDate#153, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n      +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, U AS CDC_LOAD_CODE#142, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n         +- Filter NOT (Units#177 = Units#109)\n            +- Project [SKU#108, SalesDate#107, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, LOAD_TS#124, Units#177, DIFSourceFile#178, BusinessDate#179]\n               +- Join Inner, ((SKU#108 = SKU#176) AND (SalesDate#107 = SalesDate#175))\n                  :- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, CDC_LOAD_CODE#117, to_timestamp(2024-01-15 00:00:00, Some(yyyy-MM-dd HH:mm:ss), TimestampType, Some(Asia/Kolkata), false) AS LOAD_TS#124]\n                  :  +- Project [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111, I AS CDC_LOAD_CODE#117]\n                  :     +- LogicalRDD [SalesDate#107, SKU#108, Units#109, DIFSourceFile#110, BusinessDate#111], false\n                  +- LogicalRDD [SalesDate#175, SKU#176, Units#177, DIFSourceFile#178, BusinessDate#179], false\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
