{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T10:19:00.501455Z",
     "start_time": "2025-04-17T10:19:00.498129Z"
    }
   },
   "source": [
    "# Step 1: Import the original DIFProcessedTableHelper\n",
    "from pyspark.sql import SparkSession\n",
    "import sys, datetime\n",
    "import pandas as pd\n",
    "sys.path.append(\"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/LoadTypes/\")\n",
    "sys.path.append(\"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/LoadTypes/Libs/\")\n",
    "import DIFProcessedHelper3 as DIFProcessedTableHelper3\n",
    "from DatabaseConstants import AssetColumns"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:19:00.625133Z",
     "start_time": "2025-04-17T10:19:00.515300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType,  FloatType\n",
    "import os\n",
    "\n",
    "# Mock Asset Configuration (now with Raw_Table_Name path and data types)\n",
    "mockAsset = {\n",
    "    \"Processed_Table_Name\": \"T1SubsetAppend\",\n",
    "    \"Load_Type_Code\": \"T1-Subset-Append\",  # Example load type\n",
    "    \"BusinessDate\": \"2025-04-17\",  # Example business date\n",
    "    \"IngestFile\": \"file1.csv\",  # Example source file name\n",
    "    \"Raw_Table_Name\": \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/LoadTypes/Raw/\",  # Folder for raw Parquet files\n",
    "        \"Fields\": [\n",
    "        {\"Source_Field_Name\": \"SalesDate\", \"Field_Sequence_Number\": 1, \"Target_Field_Name\": \"SalesDate\", \"IsKey_Indicator\": \"Y\", \"Source_Data_Type_Code\": \"STRING\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Source_Field_Name\": \"SKU\", \"Field_Sequence_Number\": 2, \"Target_Field_Name\": \"SKU\", \"IsKey_Indicator\": \"Y\", \"Source_Data_Type_Code\": \"STRING\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Source_Field_Name\": \"Units\", \"Field_Sequence_Number\": 3, \"Target_Field_Name\": \"Units\", \"IsKey_Indicator\": \"N\", \"Source_Data_Type_Code\": \"INTEGER\", \"Target_Data_Type_Code\": \"INTEGER\"},\n",
    "        {\"Source_Field_Name\": \"DIFSourceFile\", \"Field_Sequence_Number\": 4, \"Target_Field_Name\": \"DIFSourceFile\", \"IsKey_Indicator\": \"N\", \"Source_Data_Type_Code\": \"STRING\", \"Target_Data_Type_Code\": \"STRING\"},\n",
    "        {\"Source_Field_Name\": \"BusinessDate\", \"Field_Sequence_Number\": 5, \"Target_Field_Name\": \"BusinessDate\", \"IsKey_Indicator\": \"N\", \"Source_Data_Type_Code\": \"STRING\", \"Target_Data_Type_Code\": \"STRING\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize the Spark session for testing\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"SimulateRawTable\").getOrCreate()\n",
    "\n",
    "# Simulating raw data (mocked for testing)\n",
    "data = [\n",
    "    (\"2024-01-12\", \"A\", 100, \"file1.csv\", \"2024-01-12\"),\n",
    "    (\"2024-01-12\", \"B\", 200, \"file1.csv\", \"2024-01-12\"),\n",
    "    (\"2024-01-13\", \"A\", 150, \"file2.csv\", \"2024-01-13\"),\n",
    "    (\"2024-01-13\", \"B\", 250, \"file2.csv\", \"2024-01-13\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame with raw data\n",
    "df = spark.createDataFrame(data, [\"SalesDate\", \"SKU\", \"Units\", \"DIFSourceFile\", \"BusinessDate\"])\n",
    "\n",
    "# Step 1: Cast the columns according to the data types specified in the mockAsset\n",
    "def cast_columns_based_on_data_type(df, mockAsset):\n",
    "    for field in mockAsset[\"Fields\"]:\n",
    "        target_field = field[\"Target_Field_Name\"]\n",
    "        data_type = field[\"Target_Data_Type_Code\"]\n",
    "\n",
    "        if data_type == \"STRING\":\n",
    "            df = df.withColumn(target_field, df[target_field].cast(StringType()))\n",
    "        elif data_type == \"INTEGER\":\n",
    "            df = df.withColumn(target_field, df[target_field].cast(IntegerType()))\n",
    "        elif data_type == \"DATE\":\n",
    "            df = df.withColumn(target_field, df[target_field].cast(DateType()))\n",
    "        elif data_type == \"FLOAT\":\n",
    "            df = df.withColumn(target_field, df[target_field].cast(FloatType()))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type: {data_type} for column {target_field}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Cast fields in df according to mockAsset fields' data types\n",
    "df_casted = cast_columns_based_on_data_type(df, mockAsset)\n",
    "\n",
    "# Step 2: Save the DataFrame as Parquet under Raw folder\n",
    "raw_table_path = mockAsset[\"Raw_Table_Name\"]\n",
    "# Ensure the directory exists\n",
    "os.makedirs(raw_table_path, exist_ok=True)\n",
    "\n",
    "# Define the Parquet path where the data will be saved\n",
    "parquet_path = os.path.join(raw_table_path, \"raw_data.parquet\")\n",
    "\n",
    "# Save the DataFrame as Parquet file\n",
    "df_casted.write.mode(\"overwrite\").parquet(parquet_path)"
   ],
   "id": "e607e730875a398d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 15:49:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:22:47.042662Z",
     "start_time": "2025-04-17T10:22:47.027807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Mock the necessary configurations\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Mock the DIFLogger class to simply print logs\n",
    "class DIFLogger:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def info(self, message):\n",
    "        print(f\"INFO: {message}\")\n",
    "\n",
    "    def debug(self, message):\n",
    "        print(f\"DEBUG: {message}\")\n",
    "\n",
    "    def error(self, message):\n",
    "        print(f\"ERROR: {message}\")\n",
    "\n",
    "    def warn(self, message):\n",
    "        print(f\"WARNING: {message}\")\n",
    "\n",
    "# Initialize the SparkSession for testing\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"DIFProcessedHelperTest\").getOrCreate()\n",
    "\n",
    "# Step 3: Use the real DIFProcessedTableHelper class for testing\n",
    "class DIFProcessedTableHelperTest(DIFProcessedTableHelper3.DIFProcessedTableHelper):\n",
    "    def __init__(self, pEnvConfig, pAssetGroupConfig, pLogger, pSpark, pAsset):\n",
    "        # Call the parent constructor\n",
    "        super().__init__(pEnvConfig, pAssetGroupConfig, pLogger, pSpark, pAsset)\n",
    "\n",
    "    # Function to manually define the schema based on mockAsset fields\n",
    "    def define_schema(self, mockAsset):\n",
    "        fields = []\n",
    "        for field in mockAsset[\"Fields\"]:\n",
    "            field_name = field[\"Target_Field_Name\"]\n",
    "            data_type = field[\"Target_Data_Type_Code\"]\n",
    "\n",
    "            # Map the data type to the appropriate Spark SQL type\n",
    "            if data_type == \"STRING\":\n",
    "                fields.append(StructField(field_name, StringType(), True))\n",
    "            elif data_type == \"INTEGER\":\n",
    "                fields.append(StructField(field_name, IntegerType(), True))\n",
    "            elif data_type == \"DATE\":\n",
    "                fields.append(StructField(field_name, DateType(), True))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data type: {data_type} for field {field_name}\")\n",
    "\n",
    "        return StructType(fields)\n",
    "\n",
    "    # Manually create a Raw DataFrame inside the method\n",
    "    def getRawDataFrame(self, pTimeStamp):\n",
    "        try:\n",
    "            self.aLogger.info(\"DIFTableHelper.getRawDataFrame\")\n",
    "\n",
    "            # Define the schema for the raw data based on mockAsset fields\n",
    "            schema = self.define_schema(self.aAsset)\n",
    "\n",
    "            # Simulating the raw data (this will be done manually instead of reading a Parquet file)\n",
    "            data = [\n",
    "                (\"2024-01-12\", \"A\", 100, \"file1.csv\", \"2024-01-12\"),\n",
    "                (\"2024-01-12\", \"B\", 200, \"file1.csv\", \"2024-01-12\"),\n",
    "                (\"2024-01-13\", \"A\", 150, \"file2.csv\", \"2024-01-13\"),\n",
    "                (\"2024-01-13\", \"B\", 250, \"file2.csv\", \"2024-01-13\"),\n",
    "            ]\n",
    "\n",
    "            # Create the DataFrame with the raw data and defined schema\n",
    "            dfRaw = self.aSpark.createDataFrame(data, schema)\n",
    "\n",
    "            # Add CDC_LOAD_CODE and LOAD_TS columns\n",
    "            dfRaw = dfRaw.withColumn(\"CDC_LOAD_CODE\", lit(\"I\")) \\\n",
    "                         .withColumn(\"LOAD_TS\", lit(pTimeStamp))\n",
    "\n",
    "            self.aLogger.debug(\"This is the data from the Raw table to be processed\")\n",
    "            dfRaw.show()\n",
    "\n",
    "            self.aLogger.info(\"getRawDataFrame.End.25.02.25\")\n",
    "            return dfRaw\n",
    "        except Exception as ex:\n",
    "            self.aLogger.error(\"getRawDataFrame.Error:\" + str(ex))\n",
    "            raise Exception(str(ex))\n",
    "\n",
    "    def applyTempRecordsToTTable(self, pTimeStamp, dfTarget):\n",
    "        try:\n",
    "            self.aLogger.info(\"applyTempRecordsToTTable.Start\")\n",
    "\n",
    "            # Get the incoming adjusted data\n",
    "            dfRaw = self.getRawDataFrame(pTimeStamp)\n",
    "            dfIncoming = self.getAdjustedRawDataFrame(dfRaw)\n",
    "\n",
    "            # Get the main processed table (T Table)\n",
    "            mainTable = self.aAsset[f'{AssetColumns.Processed_Table_Name}']\n",
    "            self.aLogger.info(\"Processing table: \" + mainTable)\n",
    "            # dfTarget = self.aSpark.table(mainTable)\n",
    "\n",
    "            # Handle From/To and Updates (if applicable)\n",
    "            if self.aInstructions[\"FromToColumns\"]:\n",
    "                # Mark records for update where END_TS = '2999-12-31'\n",
    "                dfUpdates = dfIncoming.join(dfTarget, on=\"KEY_CHECKSUM_TXT\", how=\"inner\")\\\n",
    "                    .filter((dfTarget[\"END_TS\"] == \"2999-12-31\") & (dfIncoming[\"CDC_LOAD_CODE\"] == \"I\"))\n",
    "\n",
    "                dfUpdates = dfUpdates.withColumn(\"CDC_LOAD_CODE\", lit(\"U\"))\\\n",
    "                    .withColumn(\"END_TS\", lit(self.aAsset[\"BusinessDate\"]))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "                # Update the T Table with CDC_LOAD_CODE='U'\n",
    "                dfTarget = dfTarget.subtract(dfUpdates)  # Remove old data that's being updated\n",
    "                dfTarget = dfTarget.union(dfUpdates)  # Add updated data\n",
    "\n",
    "            # Handle From/To + Deletes (if applicable)\n",
    "            if self.aInstructions[\"FromToColumns\"]:\n",
    "                # Mark records for deletion where CDC_LOAD_CODE = 'I' and END_TS = '2999-12-31'\n",
    "                dfDeletes = dfIncoming.join(dfTarget, on=\"KEY_CHECKSUM_TXT\", how=\"left_anti\")\\\n",
    "                    .filter(dfTarget[\"CDC_LOAD_CODE\"] == \"I\")\n",
    "\n",
    "                dfDeletes = dfDeletes.withColumn(\"CDC_LOAD_CODE\", lit(\"D\"))\\\n",
    "                    .withColumn(\"END_TS\", lit(self.aAsset[\"BusinessDate\"]))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "                # Remove deleted records from T Table\n",
    "                dfTarget = dfTarget.subtract(dfDeletes)  # Remove records to be deleted\n",
    "                dfTarget = dfTarget.union(dfDeletes)  # Add deleted records\n",
    "\n",
    "            # Handle HashColumns: Updates and Deletes must be physically deleted (if applicable)\n",
    "            if not self.aInstructions[\"FromToColumns\"] and self.aInstructions[\"HashColumns\"]:\n",
    "                dfToDelete = dfIncoming.join(dfTarget, on=\"KEY_CHECKSUM_TXT\", how=\"inner\")\\\n",
    "                    .filter(dfIncoming[\"CDC_LOAD_CODE\"].isin(\"U\", \"D\"))\n",
    "\n",
    "                dfTarget = dfTarget.subtract(dfToDelete)  # Remove deleted or updated records\n",
    "\n",
    "            # Handle Missing Keys: Mark as Deleted (if applicable)\n",
    "            if self.aInstructions[\"Delete\"] == \"MissingKeys\":\n",
    "                missingKeys = dfTarget.join(dfIncoming, on=\"KEY_CHECKSUM_TXT\", how=\"left_anti\")\n",
    "\n",
    "                missingKeys = missingKeys.withColumn(\"CDC_LOAD_CODE\", lit(\"D\"))\\\n",
    "                    .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\\\n",
    "                    .withColumn(\"END_TS\", lit(self.aAsset[\"BusinessDate\"]))\n",
    "\n",
    "                # Update T Table for missing keys (soft delete)\n",
    "                dfTarget = dfTarget.subtract(missingKeys)  # Remove missing keys\n",
    "                dfTarget = dfTarget.union(missingKeys)  # Add missing keys as deleted\n",
    "\n",
    "            # Finally, add the new data to the T Table (Insert)\n",
    "            dfInserts = dfIncoming.withColumn(\"CDC_LOAD_CODE\", lit(\"I\"))\\\n",
    "                .withColumn(\"LOAD_TS\", to_timestamp(lit(pTimeStamp), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "            # Add the new data to T Table\n",
    "            dfTarget.printSchema()\n",
    "            dfInserts.printSchema()\n",
    "            dfTarget = dfTarget.union(dfInserts)\n",
    "\n",
    "            # Write the final DataFrame back to the Processed Table (T Table)\n",
    "            # dfTarget.write.mode(\"append\").saveAsTable(mainTable)\n",
    "\n",
    "            dfTarget.collect()  # Collect DataFrame to prevent OutOfMemoryError\n",
    "            return dfTarget\n",
    "\n",
    "            self.aLogger.info(\"applyTempRecordsToTTable.End\")\n",
    "        except Exception as ex:\n",
    "            self.aLogger.error(\"applyTempRecordsToTTable.Error: \" + str(ex))\n",
    "            raise Exception(str(ex))"
   ],
   "id": "308499128cbd20ee",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:19:00.652601Z",
     "start_time": "2025-04-17T10:19:00.650442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mockEnvConfig = {\n",
    "    \"config\": {\"some_key\": \"some_value\"}  # You can add more keys here as needed\n",
    "}\n",
    "\n",
    "# Mock asset group configuration (pAssetGroupConfig) as a dictionary\n",
    "mockAssetGroupConfig = {\n",
    "    \"AssetGroupName\": \"MockAssetGroup\",\n",
    "    \"some_config\": \"MockConfig\"  # Add other config variables as needed\n",
    "}\n",
    "\n",
    "# Create an instance of DIFLogger\n",
    "mockLogger = DIFLogger()\n",
    "\n",
    "# Create an instance of the real DIFProcessedTableHelper (subclassed for testing)\n",
    "helper = DIFProcessedTableHelperTest(mockEnvConfig, mockAssetGroupConfig, mockLogger, spark, mockAsset)\n",
    "\n",
    "# df = helper.getRawDataFrame(pTimeStamp=\"2024-01-15 00:00:00\")\n",
    "# df = helper.getAdjustedRawDataFrame(df)\n",
    "# df.show()\n",
    "\n",
    "# Step 4: Run a test example using the real class\n",
    "processedFinal = helper.applyTempRecordsToTTable(\"2024-01-15 00:00:00\", dfTarget)\n",
    "processedFinal.show()"
   ],
   "id": "6b03cde55127e1a2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T10:26:32.083278Z",
     "start_time": "2025-04-17T10:26:31.965398Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "299273b7da638bb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+-------------+------------+\n",
      "| SalesDate|SKU|Units|DIFSourceFile|BusinessDate|\n",
      "+----------+---+-----+-------------+------------+\n",
      "|2024-01-12|  A|  110|    file1.csv|  2024-01-12|\n",
      "|2024-01-13|  B|  220|    file1.csv|  2024-01-13|\n",
      "|2024-01-14|  C|  300|    file2.csv|  2024-01-14|\n",
      "+----------+---+-----+-------------+------------+\n",
      "\n",
      "INFO: applyTempRecordsToTTable.Start\n",
      "ERROR: applyTempRecordsToTTable.Error: local variable 'dfIncoming' referenced before assignment\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "local variable 'dfIncoming' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 98\u001B[0m, in \u001B[0;36mDIFProcessedTableHelperTest.applyTempRecordsToTTable\u001B[0;34m(self, pTimeStamp, dfTarget)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Get the incoming adjusted data (dfIncoming)\u001B[39;00m\n\u001B[0;32m---> 98\u001B[0m dfRaw \u001B[38;5;241m=\u001B[39m \u001B[43mdfIncoming\u001B[49m\n\u001B[1;32m     99\u001B[0m dfIncoming \u001B[38;5;241m=\u001B[39m dfRaw\n",
      "\u001B[0;31mUnboundLocalError\u001B[0m: local variable 'dfIncoming' referenced before assignment",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 182\u001B[0m\n\u001B[1;32m    179\u001B[0m helper \u001B[38;5;241m=\u001B[39m DIFProcessedTableHelperTest(mockEnvConfig, mockAssetGroupConfig, mockLogger, spark, mockAsset)\n\u001B[1;32m    181\u001B[0m \u001B[38;5;66;03m# Run the test example\u001B[39;00m\n\u001B[0;32m--> 182\u001B[0m processedFinal \u001B[38;5;241m=\u001B[39m \u001B[43mhelper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplyTempRecordsToTTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m2024-01-15 00:00:00\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdfTarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m# Show the processed result\u001B[39;00m\n\u001B[1;32m    185\u001B[0m processedFinal\u001B[38;5;241m.\u001B[39mshow()\n",
      "Cell \u001B[0;32mIn[18], line 172\u001B[0m, in \u001B[0;36mDIFProcessedTableHelperTest.applyTempRecordsToTTable\u001B[0;34m(self, pTimeStamp, dfTarget)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maLogger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplyTempRecordsToTTable.Error: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(ex))\n\u001B[0;32m--> 172\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;28mstr\u001B[39m(ex))\n",
      "\u001B[0;31mException\u001B[0m: local variable 'dfIncoming' referenced before assignment"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
