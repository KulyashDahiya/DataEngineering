{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T10:10:12.209905Z",
     "start_time": "2025-04-25T10:10:12.087341Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:10:39.270498Z",
     "start_time": "2025-04-25T10:10:13.337718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Generate Employee CSVs\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define output path\n",
    "CSVpath = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/EmployeeCSVs/\"\n",
    "\n",
    "# Sample data to choose from\n",
    "employee_names = [\"John\", \"Alice\", \"Bob\", \"Carol\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Hank\", \"Ivy\"]\n",
    "departments = [\"HR\", \"Engineering\", \"Finance\", \"Sales\", \"Marketing\", \"IT\"]\n",
    "manager_names = [\"ManagerA\", \"ManagerB\", \"ManagerC\", \"ManagerD\", \"ManagerE\"]\n",
    "\n",
    "# Generate 100 files\n",
    "for file_index in range(1, 101):\n",
    "    num_records = random.randint(100, 200)\n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        employee_id = f\"{file_index:03d}{i:04d}\"\n",
    "        employee_name = random.choice(employee_names)\n",
    "        department = random.choice(departments)\n",
    "        salary = round(random.uniform(40000, 120000), 2)\n",
    "        manager = random.choice(manager_names)\n",
    "        data.append((employee_id, employee_name, department, salary, manager))\n",
    "\n",
    "    # Create DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"EmployeeID\", StringType(), True),\n",
    "        StructField(\"EmployeeName\", StringType(), True),\n",
    "        StructField(\"DepartmentName\", StringType(), True),\n",
    "        StructField(\"Salary\", DoubleType(), True),\n",
    "        StructField(\"ManagerName\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    # Write each file individually\n",
    "    output_file = f\"{CSVpath}Employee_{file_index}.csv\"\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_file)\n",
    "\n",
    "spark.stop()"
   ],
   "id": "d89d36067f1d47c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/25 15:40:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/25 15:40:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/25 15:40:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:13:46.352193Z",
     "start_time": "2025-04-25T14:13:44.357679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random, os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save Single Parquet File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define output path\n",
    "output_path = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Singleparquet_file/Employees.parquet\"\n",
    "output_path = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/ParquetSave/Employee.parquet\"\n",
    "\n",
    "# Sample values\n",
    "names = [\"John\", \"Alice\", \"Bob\", \"Carol\", \"David\", \"Eva\", \"Frank\", \"Grace\", \"Hank\", \"Ivy\"]\n",
    "departments = [\"HR\", \"Engineering\", \"Finance\", \"Sales\", \"Marketing\", \"IT\"]\n",
    "managers = [\"ManagerA\", \"ManagerB\", \"ManagerC\", \"ManagerD\", \"ManagerE\"]\n",
    "\n",
    "# Generate 5000 records\n",
    "data = []\n",
    "for i in range(1, 5001):\n",
    "    emp_id = f\"E{i:05d}\"\n",
    "    name = random.choice(names)\n",
    "    dept = random.choice(departments)\n",
    "    salary = round(random.uniform(40000, 120000), 2)\n",
    "    manager = random.choice(managers)\n",
    "    data.append((emp_id, name, dept, salary, manager))\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"DepartmentName\", StringType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"ManagerName\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Save as a single Parquet file\n",
    "# df.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# df.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "spark.stop()"
   ],
   "id": "4a2b58dd59708961",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 19:43:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/25 19:43:44 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T14:01:04.832395Z",
     "start_time": "2025-04-25T14:01:04.709285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tempParquetFolder = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Singleparquet_file_temp/\"\n",
    "\n",
    "output_path = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Parquet/\"\n",
    "\n",
    "finalParquetFile = os.path.join(output_path, \"Employee_12345.parquet\")\n",
    "\n",
    "tempParquetFile = \"/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Singleparquet_file/Employees.parquet/part-00000-95fa355e-cce9-44fb-b2a4-3cacf763bfba-c000.snappy.parquet\"\n",
    "\n",
    "shutil.move(tempParquetFile, finalParquetFile)\n",
    "\n",
    "# Move the single Parquet part file to the final destination\n",
    "# for temp_file in os.listdir(tempParquetFolder):\n",
    "#     if temp_file.endswith(\".parquet\"):\n",
    "#         shutil.move(os.path.join(tempParquetFolder, temp_file), finalParquetFile)\n",
    "#         print(f\"Moved {temp_file} to {finalParquetFile}\")\n",
    "\n",
    "# Clean up the temporary folder\n",
    "# shutil.rmtree(tempParquetFolder)"
   ],
   "id": "bc6274bb919a55fa",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Parquet/Employee_12345.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:806\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 806\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrename\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Singleparquet_file/Employees.parquet/part-00000-95fa355e-cce9-44fb-b2a4-3cacf763bfba-c000.snappy.parquet' -> '/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Parquet/Employee_12345.parquet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m finalParquetFile \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(output_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmployee_12345.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      7\u001B[0m tempParquetFile \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Singleparquet_file/Employees.parquet/part-00000-95fa355e-cce9-44fb-b2a4-3cacf763bfba-c000.snappy.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtempParquetFile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinalParquetFile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Move the single Parquet part file to the final destination\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# for temp_file in os.listdir(tempParquetFolder):\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#     if temp_file.endswith(\".parquet\"):\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Clean up the temporary folder\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# shutil.rmtree(tempParquetFolder)\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:826\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    824\u001B[0m         rmtree(src)\n\u001B[1;32m    825\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 826\u001B[0m         \u001B[43mcopy_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    827\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(src)\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m real_dst\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:435\u001B[0m, in \u001B[0;36mcopy2\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    433\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(dst):\n\u001B[1;32m    434\u001B[0m     dst \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dst, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(src))\n\u001B[0;32m--> 435\u001B[0m \u001B[43mcopyfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_symlinks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_symlinks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m copystat(src, dst, follow_symlinks\u001B[38;5;241m=\u001B[39mfollow_symlinks)\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dst\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:264\u001B[0m, in \u001B[0;36mcopyfile\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    262\u001B[0m     os\u001B[38;5;241m.\u001B[39msymlink(os\u001B[38;5;241m.\u001B[39mreadlink(src), dst)\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(src, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fsrc, \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fdst:\n\u001B[1;32m    265\u001B[0m         \u001B[38;5;66;03m# macOS\u001B[39;00m\n\u001B[1;32m    266\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m _HAS_FCOPYFILE:\n\u001B[1;32m    267\u001B[0m             \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/kulyashdahiya/STUDY/DataEngineering/PysparkLearning/Basics/ELC_Try/TestFilesCreation/Parquet/Employee_12345.parquet'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def processCSVAsset(self, preStagingFolder, batchID, dAsset, sFolder):\n",
    "    \"\"\"\n",
    "    Processes a single CSV asset:\n",
    "    - Reads CSV files from Pre-Staging.\n",
    "    - Saves them as Parquet in the Staging folder.\n",
    "    \"\"\"\n",
    "    inputPattern = os.path.join(preStagingFolder, dAsset[\"batchIngestFilePattern\"])  # Path to CSV files\n",
    "    self.aLogger.info(f\"Preprocessing CSV for asset: {dAsset['assetName']} using pattern: {inputPattern}\")\n",
    "\n",
    "    # Construct the temporary Parquet save path\n",
    "    tempParquetFolder = os.path.join(sFolder, f\"{dAsset['assetName']}_{batchID}_temp\")\n",
    "\n",
    "    # Step 1: Read the CSV files into a DataFrame\n",
    "    df = self.aSpark.read.option(\"header\", \"true\").csv(inputPattern)\n",
    "\n",
    "    # Step 2: Write the DataFrame as Parquet in the temporary folder\n",
    "    self.aLogger.info(f\"Saving Parquet temporarily for asset: {dAsset['assetName']} to {tempParquetFolder}\")\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(tempParquetFolder)\n",
    "\n",
    "    # Step 3: Move the Parquet file to the final destination and rename it\n",
    "    tempParquetFile = os.path.join(tempParquetFolder, \"part-00000-*.parquet\")  # The part file inside the temporary folder\n",
    "    finalParquetFile = os.path.join(sFolder, f\"{dAsset['assetName']}_{batchID}.parquet\")\n",
    "\n",
    "    try:\n",
    "        # Move the single Parquet part file to the final destination\n",
    "        for temp_file in os.listdir(tempParquetFolder):\n",
    "            if temp_file.endswith(\".parquet\"):\n",
    "                shutil.move(os.path.join(tempParquetFolder, temp_file), finalParquetFile)\n",
    "                self.aLogger.info(f\"Moved {temp_file} to {finalParquetFile}\")\n",
    "\n",
    "        # Clean up the temporary folder\n",
    "        shutil.rmtree(tempParquetFolder)\n",
    "\n",
    "        # Update the asset with the new Parquet path\n",
    "        dAsset[\"ingestFile\"] = finalParquetFile\n",
    "        self.aLogger.info(f\"CSV asset {dAsset['assetName']} preprocessed and saved to {finalParquetFile}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        self.aLogger.error(f\"Error moving Parquet file {tempParquetFile} to {finalParquetFile}: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ],
   "id": "4cf8f3ddce408aff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
